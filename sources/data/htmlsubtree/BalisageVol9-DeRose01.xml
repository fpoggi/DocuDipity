<div id="mainContainerTOC">
   <div id="mainContainerTitleTOC" onclick="$('#mainContainerEntriesTOC').toggle('1000');">Table of Content</div>
   <div id="mainContainerEntriesTOC" style="display:none;">
      <div class="headedContainerTOC">
         <div class="headedContainerTitleTOC"><a href="#ThestructureofcontentANCHOR" name="ThestructureofcontentTOC">The structure of content</a></div>
         <div class="headedContainerTOC">
            <div class="headedContainerTitleTOC"><a href="#IntroductionANCHOR" name="IntroductionTOC">Introduction</a></div>
         </div>
         <div class="headedContainerTOC">
            <div class="headedContainerTitleTOC"><a href="#Tediousandbrief:LanguagevsMarkup?ANCHOR" name="Tediousandbrief:LanguagevsMarkup?TOC">Tedious and brief: Language vs. Markup?</a></div>
         </div>
         <div class="headedContainerTOC">
            <div class="headedContainerTitleTOC"><a href="#TextAnalyticstotherescue?ANCHOR" name="TextAnalyticstotherescue?TOC">Text Analytics to the rescue?</a></div>
            <div class="headedContainerTOC">
               <div class="headedContainerTitleTOC"><a href="#StatisticalmethodsANCHOR" name="StatisticalmethodsTOC">Statistical methods</a></div>
            </div>
            <div class="headedContainerTOC">
               <div class="headedContainerTitleTOC"><a href="#IntuitivemethodsANCHOR" name="IntuitivemethodsTOC">Intuitive methods</a></div>
            </div>
            <div class="headedContainerTOC">
               <div class="headedContainerTitleTOC"><a href="#Accuracy?ANCHOR" name="Accuracy?TOC">Accuracy?</a></div>
            </div>
         </div>
         <div class="headedContainerTOC">
            <div class="headedContainerTitleTOC"><a href="#TextAnalytics'RelationtoMarkupANCHOR" name="TextAnalytics'RelationtoMarkupTOC">Text Analytics' Relation to Markup</a></div>
         </div>
         <div class="headedContainerTOC">
            <div class="headedContainerTitleTOC"><a href="#Howshallwefindtheconcordofthisdiscord?ANCHOR" name="Howshallwefindtheconcordofthisdiscord?TOC">How shall we find the concord of this discord?</a></div>
         </div>
         <div class="headedContainerTOC">
            <div class="headedContainerTitleTOC"><a href="#BibliographyANCHOR" name="BibliographyTOC">Bibliography</a></div>
         </div>
      </div>
   </div>
</div>
<div id="mainContainerTERMS">
   <div id="mainContainerTitleTERMS" onclick="$('#mainContainerEntriesTERMS').toggle('1000')">Index of Terms</div>
   <div id="mainContainerEntriesTERMS" style="display:none;">
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">a</div>
         <div class="singletermTERMS">about</div>
         <div class="singletermTERMS">August 6, 2012</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">b</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">c</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">d</div>
         <div class="singletermTERMS">Director of R&amp;D</div>
         <div class="singletermTERMS">DeRose</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">e</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">f</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">g</div>
         <div class="singletermTERMS">guibutton, keycap, menuchoice</div>
         <div class="singletermTERMS">guiItem</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">h</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">i</div>
         <div class="singletermTERMS">is</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">j</div>
         <div class="singletermTERMS">J.</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">k</div>
         <div class="singletermTERMS">keycap</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">l</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">m</div>
         <div class="singletermTERMS">Markup structure</div>
         <div class="singletermTERMS">Making Hypermedia Work</div>
         <div class="singletermTERMS">Markup Systems</div>
         <div class="singletermTERMS">Markup Theory</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">n</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">o</div>
         <div class="singletermTERMS">Orthographic structure</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">p</div>
         <div class="singletermTERMS">precision</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">q</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">r</div>
         <div class="singletermTERMS">recall</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">s</div>
         <div class="singletermTERMS">Semantic structure</div>
         <div class="singletermTERMS">Syntactic structure</div>
         <div class="singletermTERMS">sderose@acm.org</div>
         <div class="singletermTERMS">Steven</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">t</div>
         <div class="singletermTERMS">Text Analytics</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">u</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">v</div>
         <div class="singletermTERMS">via</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">x</div>
         <div class="singletermTERMS">xml:lang</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">y</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">z</div>
      </div>
   </div>
</div>
<div id="mainContainerIML"><a name="ThestructureofcontentANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="article e0" version="5.0-subset Balisage-1.3" xml:id="HR-23632987-8973">
      <div class="title e1">The structure of content</div>
      <div class="info e2">
         <div class="confgroup e3">
            <div class="conftitle e4">International Symposium on Quality Assurance and Quality Control in XML</div>
            <div class="confdates e5">August 6, 2012</div>
         </div>
         <div class="abstract e6">
            <div class="para e7">Text analytics involves extracting features of meaning from natural language texts
               and making them explicit, much as markup does. It uses linguistics, AI, and statistical
               methods to get at a level of "meaning" that markup generally does not: down in the
               leaves of what to XML may be unanalyzed "content". This suggests potential for new
               kinds of error, consistency, and quality checking. However, text analytics can also
               discover features that markup 
               <div class="emphasis e8" role="ital">is</div> used for; this suggests that text analytics can also contribute to the markup process
               itself.
            </div>
            <div class="para e9">Perhaps the simplest example of text analytics' potential for checking, is xml:lang.
               Language identification is well-developed technology, and xml:lang attributes "in
               the wild" could be much improved. More interestingly, the distribution of named entities
               (people, places, organizations, etc.), topics, and emphasis interacts closely with
               documents' markup structures. Summaries, abstracts, conclusions, and the like all
               have distinctive features which can be measured. 
            </div>
            <div class="para e10">This paper provides an overview of how text analytics works, what it can do, and how
               that relates to the things we typically mark up in XML. It also discuss the trade-offs
               and decisions involved in just what we choose to mark up, and how that interacts with
               automation. It presents several specific ways that text analytics can help create,
               check, and enhance XML components, and exemplifies some cases using a high-volume
               analytics tool.
            </div>
         </div>
         <div class="author e11">
            <div class="personname e12">
               <div class="firstname e13">Steven</div>
               <div class="othername e14">J.</div>
               <div class="surname e15">DeRose</div>
            </div>
            <div class="personblurb e16">
               <div class="para e17">Steve DeRose has been working with electronic document systems since joining Andries
                  van Dam's FRESS project in 1979. He holds degrees in Computer Science and in Linguistics
                  and a Ph.D. in Computational Linguistics from Brown University. His development of
                  fast, accurate part-of-speech tagging methods for English and Greek corpora helped
                  touch off the shift from heuristic to statistical methods in computational linguistics.
               </div>
               <div class="para e18">He co-founded Electronic Book Technologies to build the first SGML browser and retrieval
                  system, "DynaText", and has been deeply involved in document standards including XML,
                  TEI, HyTime, HTML 4, XPath, XPointer, EAD, Open eBook, OSIS, NLM and others. He has
                  served as Chief Scientist of Brown University's Scholarly Technology Group and Adjunct
                  Associate Professor of Computer Science. He has written many papers, two books, and
                  eleven patents. Most recently he joined OpenAmplify, a text analytics company that
                  does very high-volume analysis of texts, mostly from social media.
               </div>
            </div>
            <div class="affiliation e19">
               <div class="jobtitle e20">Director of R&amp;D</div>
               <div class="orgname e21">OpenAmplify</div>
            </div>
            <div class="email e22">sderose@acm.org</div>
         </div>
         <div class="legalnotice e23">
            <div class="para e24">Copyright Â© 2012 by the author. Used with permission.</div>
         </div>
         <div class="keywordset e25" role="author">
            <div class="keyword e26">Text Analytics</div>
            <div class="keyword e27">Markup Systems</div>
            <div class="keyword e28">Markup Theory</div>
         </div>
      </div><a name="IntroductionANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="section e29">
         <div class="title e30">Introduction</div>
         <div class="para e31">The basic purpose of markup is to make the implicit structure of texts explicit; the
            same is true of "text analytics" (or "TA"), a fast-growing application area involving
            the automated extraction of (some) meaning from documents. In this paper I will try
            to place markup and analytics in a larger frame, and discuss their relationship to
            each other and to the notion of explicit vs. implicit information.
         </div>
         <div class="para e32">To begin, note that 'implicit' and 'explicit' are matters of degree. Language itself
            is an abstract system for thought, entirely implicit in the relevant sense here. The
            words and structures of natural language make thought more explicit, fixing it in
            the tangible medium of sound (of course, some people cannot use that medium, but still
            use language). Writing systems take us another step: particularly in their less familiar,
            non-phonetic features: sentence-initial capitals, quotation marks, ellipses, etc.
            all make linguistic phenomena explicit, and it has long been noted that punctuation
            (not to mention layout) are kinds of "markup"[
            <div class="xref e33" linkend="Coom87"></div>], albeit often ambiguous. In a yet broader sense, the "s" on the end of a plural
            noun can be considered "markup" for a semantic feature we call "plural", and a space
            is a convenient abbreviation for &lt;word&gt;. XML markup thus fills a "metalinguistic"
            role analogous to punctuation, though (ideally) richer and less ambiguous.
         </div>
         <div class="para e34">'Documents' is also an imprecise term. Here I am focusing only on the more traditional
            sense: human-readable discourses, in a (human) language, with typically near-total
            ordering. This doesn't preclude including pictures, data tables, graphs, video, models,
            etc.; but I am for now excluding documents that are mainly for machine consumption,
            or are in a specialized "language" such as for mathematics, vector graphics, database
            tables, etc.
         </div>
         <div class="para e35">What, then, can text analytics do for XML? What are some phenomena in texts that we
            might make explicit, or check, using text analytics methods? How do these mesh with
            how our linguistic and markup sytems are organized? An anonymous reviewer raised the
            valuable question of where text analytics can be effective for auto-tagging material;
            which in turn poses an old problem in a new guise: If you can use a program to find
            instances of some textual feature, why mark it up? 
         </div>
      </div><a name="Tediousandbrief:LanguagevsMarkup?ANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="section e36">
         <div class="title e37">Tedious and brief: Language vs. Markup?</div>
         <div class="para e38">To explore these questions we may arrange reality into successive levels of abstraction
            and explicitness. This is similar to the typical organization of layers in natual
            language processing systems.
         </div>
         <div class="itemizedlist e39">
            <div class="listitem e40">
               <div class="para e41">
                  <div class="emphasis e42" role="ital">Semantic structure</div>: implicit (in that we rarely "see" semantic structure directly); presumably relatively
                  unambiguous. 
               </div>
               <div class="para e43">When representing semantics, we tend to use artificial, formal languages such as predicate
                  calculus, or model the real world with semantic structures such as Montague's[
                  <div class="xref e44" linkend="Mont73"></div>], involving 
                  Entities (people, places, things);
                  Relationships (A does V to B with C near D because R...); and
                  Truth values (characterizing, say, the meaning of a sentence).
               </div>
            </div>
            <div class="listitem e45">
               <div class="para e46">
                  <div class="emphasis e47" role="ital">Syntactic structure</div>: mostly implicit, but with some explicit markers); ambiguous. "The six armed men
                  held up the bank" is a classic example of syntactic and lexical ambiguity.
               </div>
               <div class="para e48">Typical syntactic units include Morphemes -&gt; Words -&gt; Phrases -&gt; Clauses -&gt; Sentences
                  -&gt; Discourse units (such as requests, questions, and arguments), etc. At this level
                  we begin to see some explicit markers. In speech they include timing, tone contours,
                  and much more; in writing punctuation and whitespace often mark these kinds of units.
               </div>
               <div class="para e49">A full analysis of speech must also deal with many features we tend to ignore with
                  written language: incomplete or flatly incorrect grammar; back-tracking; background
                  noise that obscures the "text" much as coffee stains obscure manuscripts; even aphasic
                  speech; but we'll ignore most of that here.
               </div>
            </div>
            <div class="listitem e50">
               <div class="para e51">
                  <div class="emphasis e52" role="ital">Orthographic structure</div>: explicit (by definition); but still ambiguous. Characters in characeter sets, need
                  not map trivially to characters as linguistic units (ligatures, CJK unification, etc).
                  More significantly, orthographic choices can communicate sociolinguistics messages:
                  Hiragana vs. Katakana; italics for foreign words; even the choice of fonts. Case distinctions
                  often indicate part of speech. And a given spelling may represent different words:
                  "refuse", "tear", "lead", "dove", "wind", "console", "axes"[
                  <div class="xref e53" linkend="het1"></div>, 
                  <div class="xref e54" linkend="het2"></div>]. These are "heteronyms"; true "homonyms" (which also share pronunciation), such
                  as "bat", are rarer. Both are commonly misused, making them obvious candidates for
                  automatic checking.
               </div>
               <div class="para e55">Most English punctuation marks also have multiple uses. Ignoring numeric uses, periods
                  may end or be in the midst of sentences, or stand open abbreviations. Colon can express
                  phrase or clause or sentence boundary. The lowly apostrophe, introduced to English
                  in the 16th century, can mean open or close 'quote'; there's a possessive present;
                  contraction as in "fo'c's'le"; or sometimes plural as in P's and Q's. Punctuation
                  is, I think, underappreciated in both markup and text analytics.
                  
               </div>
               <div class="para e56">All these level involve both markup-like and linguistic-like features, so it seems
                  clear that there is much potential for synergy. At the same time, at each level the
                  nature of errors to be detected and of markup to be automated differ, and so the applications
                  of text analytics msut be considered pragmatically.
               </div>
            </div>
            <div class="listitem e57">
               <div class="para e58">
                  <div class="emphasis e59" role="ital">Markup structure</div>: explicit and uambiguous. Markup is rarely considered in natural-languages processing,
                  even though it has obvious implications. Consider tags such as &lt;del&gt; and &lt;ins&gt;, &lt;abbr&gt;,
                  as well as tags that change appropriate syntactic expectations: &lt;q&gt;, &lt;heading&gt;, &lt;code&gt;,
                  and many others. We may hope that never anything can be amiss, when simpleness and
                  duty tender it. But markup directly affects the applicability and techniques of NLP.
               </div>
            </div>
         </div>
         <div class="para e60">In large-scale text projects, the creation of high-quality markup is one of the largest
            expenses, whether measured by time, money, or perhaps even frustration. How do we
            choose what to mark up? Can text analytics shift the cost/benefit calculus, so that
            we can (a) mark up useful features we couldn't before, and (b) assure the quality
            of the features we have marked up?
         </div>
      </div><a name="TextAnalyticstotherescue?ANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="section e61">
         <div class="title e62">Text Analytics to the rescue?</div>
         <div class="para e63">Text analytics tries to extract specific features of meaning from natural language
            text (i.e., documents). It mixes linguistics, statistical and machine learning, AI,
            and the like, to do a task very much like the task of marking up text: searching through
            documents and trying to categorize meaningful pieces. Like markup, this can be done
            by humans (whose consensus, when achievable, is deemed the "gold standard" against
            which algorithms are measured), or by algorithms of many types.
         </div>
         <div class="para e64">Text analytics today seeks very specific features; largely ones that are saleable;
            it does not typically aim at a complete analysis of linguistic or pragmatic meaning.
            Among the features TA systems typically seek are:
         </div>
         <div class="itemizedlist e65">
            <div class="listitem e66">
               <div class="para e67">Language identification</div>
            </div>
            <div class="listitem e68">
               <div class="para e69">Genre categorization: Is this reportage, advocacy, forum discussions, reviews, ads,
                  spam,...).
               </div>
            </div>
            <div class="listitem e70">
               <div class="para e71">Characteristics of the author: gender, age, education level,....</div>
            </div>
            <div class="listitem e72">
               <div class="para e73">Topic disambiguation: Flying rodents vs. baseball implements; non-acids vs. other
                  baseball implements; mispelled photographs vs. baseball players vs. carafes; and so
                  on.
               </div>
            </div>
            <div class="listitem e74">
               <div class="para e75">Topic weight: Is the most important topic here baseball, or sports, or the Acme Mark-12
                  jet-assisted fielding glove's tendency to overheat?
               </div>
            </div>
            <div class="listitem e76">
               <div class="para e77">Sentiment: Does the author like topic X?</div>
            </div>
            <div class="listitem e78">
               <div class="para e79">Intentions: Is the user intent on buying/selling/getting help?</div>
            </div>
            <div class="listitem e80">
               <div class="para e81">Times and events: Is the text reporting or announcing some event? What kind? When?</div>
            </div>
            <div class="listitem e82">
               <div class="para e83">"Style" or "tone" measures: Decisiveness, flamboyance, partisan flavor, etc.</div>
            </div>
         </div>
         <div class="para e84">Text analytics is related to many more traditional techniques, such as text summarization,
            topical search, and the like. However, it tends to be more focused on detecting very
            specific features, and operating over very large collections such as Twitter feeds,
            FaceBook posts, and the like. The OpenAmplify[
            <div class="xref e85" linkend="oa"></div>] service, for example, regularly analyzes several million documents per day.
         </div>
         <div class="para e86">A natural thing for a text analysis system to do, is to mark up parts of documents
            with what features they were found to express: this "he" and that "our renowned Duke"
            are references to the "Theseus" mentioned at the beginning; this paragraph is focused
            on fuel shortages and the author appears quite angry about them; and so on. Of course,
            some features only emerge from the text as a whole: the text may be about space exploration,
            even though that topic is never mentioned by name.
         </div>
         <div class="para e87">Users of text analytics commonly want to process large collections of short social
            media texts, searching for particular kinds of mentions. For example, Acme Mustardseed
            Company may want to know whenever someone posts about them, and what attitude is expressed.
            Beyond mere positive vs. negative, knowing that some posters are advocates or detractors
            (advising others to buy or avoid), can facilitate better responses. Other companies
            may want to route emails to tech support vs. sales, or measure the response to a marketing
            campaign, or find out what specific features of their product are popular.
         </div>
         <div class="para e88">Text analytics algorithms are varied and sometimes compex, but there are two overall
            approaches:
         </div>
         <div class="para e89">One method is almost purely statistical: gather a collection of texts that have been
            human-rated for some new feature, and then search for the best combination of features
            for picking out the cases you want. This "Machine Learning" approach allows for fast
            bootstrapping, even for multiple languages, and is sometimes very accurate. However,
            since it does not take much account of language structure, complex syntax, negation,
            and almost any kind of subtlety may trip it up. It's also very hard to fix manually
            -- if you go tweaking the statistically-derived parameters, unintended consequences
            show up.
         </div>
         <div class="para e90">The other general method is heuristic or intuitive: linguists analyze the feature
            in question, and find vocabulary, syntactic patterns, and so on that express it. Then
            you run a pattern-matching engine over documents. The plusses and minuses of this
            approach are opposite those of machine learning: It is much more time- and expertise-intensive;
            if the analysts are good the results can be amazing. But it's hard to do this for
            100 languages. When problems crop up, the linguists can add new and better patterns,
            add information to the lexicon, etc.
         </div>
         <div class="para e91">As Klavans and Resnik point out[
            <div class="xref e92" linkend="Kla96"></div>], it can be very effective to combine these approaches. One way to do that is to
            use statistal methods as a discovery tool, facilitating and checking experts' intuitions.
         </div>
         <div class="para e93">With either approach or a combination, you end up with an algorithm, or more abstractly
            a function, that takes a text and returns some rating of how likely the text is to
            exhibit the phenomenon you're looking for. For the trivial 
         </div><a name="StatisticalmethodsANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="section e94">
            <div class="title e95">Statistical methods</div>
            <div class="para e96">These methods, as noted, involve combining many features to characterize when the
               sought. Usually the features are very simple, so they can be detectected reliably
               and very quickly: frequencies of words or specific function-words, character-pair
               frequencies, sentence and word lengths, frequency of various punctuation, and so on.
            </div>
            <div class="para e97">The researcher may choose some features to try based on intuition, but it is also
               common simply to throw in a wide variety of features, and see what works. The features
               just listed turn out to be commonly useful as well as convenient.
            </div>
            <div class="para e98">Taking again the simple example of language identification, one might guess that the
               overall frequencies of individual characters would suffice. To test this hypothesis,
               one collects from hundreds to perhaps a million texts, categorized (in this example)
               by language. Say, all the English texts in one folder, and all the French texts in
               another. Generating such a "corpus" is usually the critical-path item: somehow you
               must sort the texts out by the very phenomenon you don't yet have a tool for. This
               can be done by the researcher (or more likely their assistants), by Mechanical Turk
               users, or perhaps by a weak automatic classifier with post-checking; sometimes an
               appropriate corpus can be found rather than constructed de novo. Constructing an annotated
               corpus has much in common with the sometimes difficult and expensive task of doing
               XML markup, particularly in a new domain where you must also develop and refine schemas
               and best-practice tagging guidelines and conventions.
            </div>
            <div class="para e99">Given such a "gold standard" corpus, software runs over each text to calculate the
               features (in this example by counting characters). This produces a list of numbers
               for each text. The mathematically-inclined reader will have noticed that such a list
               is a vector, or a position in space -- only the space may have hundreds or thousands
               of dimensions, not merely 11. For ease of intuition, imagine using only 3 features,
               such as the frequencies of "k" and "?" and a flag for whether the text is from Twitter.
               Usually, frequency measures are normalized by text length, so that texts of widely
               varying lengths are comparable; and binary features such as being from Twitter, are
               treated as 0 or 1.
            </div>
            <div class="para e100">Each document's list of 3 numbers equates to a location in normal space: (x, y, z).
               It is easy to calculate the distance between any two such points, and this is a measure
               of "distance" or similarity between the documents those two points represent. The
               "as the crow flies" distance is often used, but there are other useful measures such
               as the "Manhatten distance", the "cosine distance", and others.
            </div>
            <div class="para e101">Software such as WEKA[
               <div class="xref e102" linkend="WEKA"></div>] runs through all the vectors, and tries to find the best combination of features
               to look at, in order to correctly assign documents to the target categories. This
               is called "training". Of these 3 features, the frequency of "k" is typically much
               higher in English texts than French texts, while the other 2 features don't contribute
               much (that is, they are not distinctive for this purpose). Such softare typically
               results in a "weight" for each feature for each language: in this case "k-" frequency
               would have a substantial positive weight for English, and negative for French.
            </div>
            <div class="para e103">Intuitively, this training allows you to discard irrelevant featurs, and to weight
               the relevant features so as to maximize the number of texts that will be accurately
               categorized. The real proof comes when you try out the results on texts that were
               not part of the original training corpus, and you discover whether the training text
               were really representative or not. With too small a corpus or with features that really
               aren't appropriate, you may get seemingly good results from training, that don't actually
               work in the end.
            </div>
            <div class="para e104">This example is a bit too simple -- as it happens, counting sequences of 2 or 3 letters
               characterizes specific languages far better than single letters. Abramson[
               <div class="xref e105" linkend="Abr63"></div>] (pp. 33-38) presents text generated randomly, but in accordance with tables of such
               counts (known as "Hidden Markov Models") based on various languages. For example,
               if the last two letters generated were "mo", the next letter would be chosen according
               to how often each letter occurs following "mo". Abramson's randomly generated texts
               illustrate how well even so trivial a model distinguish languages:
            </div>
            <div class="itemizedlist e106">
               <div class="listitem e107">
                  <div class="para e108">(1)	jou mouplas de monnernaissains deme us vreh bre tu de toucheur dimmere lles mar
                     elame re a ver il douvents so
                  </div>
               </div>
               <div class="listitem e109">
                  <div class="para e110">(2)	bet ereiner sommeit sinach gan turhatt er aum wie best alliender taussichelle
                     laufurcht er bleindeseit uber konn
                  </div>
               </div>
               <div class="listitem e111">
                  <div class="para e112">(3)	rama de lla el guia imo sus condias su e uncondadado dea mare to buerbalia nue
                     y herarsin de se sus suparoceda
                  </div>
               </div>
               <div class="listitem e113">
                  <div class="para e114">(4)	et ligercum siteci libemus acerelin te vicaescerum pe non sum minus uterne ut
                     in arion popomin se inquenque ira
                  </div>
               </div>
            </div>
            <div class="para e115">These texts are clearly identifiable as to the language on whose probabilities each
               was based.
            </div>
            <div class="para e116">Given some set of available features, there are many specific statistical methods
               that programs like WEKA can use to derive an effective model. Among them are Support
               Vector Models (SVM), simulated annealing, Bayesian modeling, and a variety of more
               traditional statistics such as regressions. The first of these is graphically intuitive:
               the program tries to position a plane in space, with all (or as many as possible)
               of the French texts falling on one side, and the English texts on the other. Some
               methods (such as SVM) can only learn "pairwise" distinctions (such as "English or
               not" and "French or not"); others can distinguish multiple categories at once (English
               or French or Spanish...). Sometimes a degree of certainty or confidence can also be
               assigned as well.
            </div>
            <div class="para e117">These methods often work quite well, and (given a training corpus) can be tried out
               rapidly. If the method fails, adding new features lets you try again. Programs can
               typically manage hundreds to a few thousand features. This seems generous, but remember
               that if you want to track the frequencies of all English words, or even all letter-triples,
               you'll quickly exceed that, so some restraint is necessary.
            </div>
            <div class="para e118">On the other hand, statistical methods are not very intuitive. Sometimes the results
               are obvious: the frequency of accented vowels is vastly higher in French than English.
               But often it is hard to see why some combination of features works, and this can make
               it hard to "trust" the resulting categorizer. This may be reminiscent of the methods
               used for norming some psychological tests, by asking countless almost random questions
               of many people, and seeing which ones correlate with various diagnoses. This can work
               very well, and is hard to accuse of bias; on the other hand, if it stops working in
               a slightly larger sample space, that may be hard to notice or repair.
            </div>
         </div><a name="IntuitivemethodsANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="section e119">
            <div class="title e120">Intuitive methods</div>
            <div class="para e121">A more "traditional" approach to developing text analytics systems is for experts
               to articulate how they would go about categorizing documents or detecting the desired
               phenomena and then implementing those methods. This is usually done as an iterative
               process, running the implementation against a corpus much as with statistical methods
               and then studying the results and refining.
            </div>
            <div class="para e122">For example, a linguist might articulate a set of patterns to use to detect names
               of educational institutions in text. One might be "any sequence of capitalized words,
               the first or last of which is one of "College", "University", "Institute", "Seminary",
               or "Academy". This rule can be easily tried on some text, and two checks can be done:
            </div>
            <div class="para e123">1: checking what it finds, in order to discover false positive such as "College Station,
               TX"; and
            </div>
            <div class="para e124">2: check what is left over (perhaps just series of capitalized words in the leftovers),
               in order to discover false negatives such as "State University of New York", "College
               of the Ozarks".
            </div>
            <div class="para e125">The rules are then refined, for example to allow uncapitalized words like "of", "at",
               and "the"; and to allow "State", "City", and "National" preceding the already-listed
               possible starter words. Extending rules by searching for words that occur in similar
               contexts also helps.
            </div>
            <div class="para e126">Eventually this process should produce a pretty good set of rules, although in natural
               language the rules will often have to include lists of idiosyncratic cases: "Harvard",
               "Columbia", "McGill", "Brown" (with the difficulty of sorting out uses as surnames,
               colors, place names, and the like).
            </div>
            <div class="para e127">Intuition-based methods tend to be far better when larger phenomena matter. For example,
               the ever-popular "Sentiment" calculation is highly sensitive to negation, and natural
               languages have a huge variety of ways to negate things. Besides the many ways overt
               negatives like "not" can be used, there are antonyms, sarcasm, condemnation by faint
               praise, and countless other techniques. Statistical methods are unlikely to work well
               for negation, in part because "not" or other signals of negation may occur quite a
               distance from what they modify; just having "not" in a sentence tells you little about
               what specifically is being negated. "There's seldom a paucity of evidence that could
               preclude not overlooking an ersatz case of negative polarity failing to be missing."
            </div>
            <div class="para e128">Intuitive approaches have the advantage of being more easily explained and more easily
               enhanced when shortcomings arise. But building them requires human rather than machine
               effort (especially difficult if one needs to support many languages).
            </div>
            <div class="para e129">Intuitive methods have the added cost and benefit, that experts commonly refer to
               high-level, abstract linguistic notions. In more realistic cases than the last example,
               a linguist might want to create rules involving parts of speech, clause boundaries,
               active vs. passive sentences, and so on. To do this requires a bit of recursion: how
               do you detect *those* phenomena in the first place? That requires some tool that can
               identify those constructs, and make them available as features for the next "level".
            </div>
            <div class="para e130">"Shallow parsing" is well understood and readily available (such as via [
               <div class="xref e131" linkend="Gate"></div>] and [
               <div class="xref e132" linkend="NLTK"></div>]), and can provide many of those features. "Shallow parsers" identify parts of speech
               (using far more than the traditional 8 distinctions), as well as relatively small
               components such as noun phrases, simple predicates, and so on, often using grammars
               constructed from rules broadly similar to those in XML schemas. Shallow parsing draws
               the line about at clauses: subordinate clauses as in "She went to the bank that advertised
               most" are very complex in the general case, and attaching them correctly to the major
               clause they occur in even more so.
            </div>
            <div class="para e133">The results of shallow parsing are usually strongly hierarchical, since they exclude
               many of the complex long-distance phenomena in language (such as relationships between
               non-adjacent nouns, pronouns, clauses, etc.). Because of this, XML is commonly used
               to represent natural-language parser output. However, this is separate from potential
               uses of text analytics on general XML. An example of the structures produced by a
               shallow parser:
            </div>
            <div class="programlisting e134" xml:space="preserve">
               
               &lt;s&gt;
               &lt;nps&gt;
               &lt;at base="the"&gt;The&lt;/at&gt;
               &lt;nn base="course"&gt;course&lt;/nn&gt;
               &lt;/nps&gt;
               &lt;pp&gt;
               &lt;in base="of"&gt;of&lt;/in&gt;
               &lt;np&gt;
               &lt;jj base="true"&gt;true&lt;/jj&gt;
               &lt;nn base="love"&gt;love&lt;/nn&gt;
               &lt;/np&gt;
               &lt;/pp&gt;
               &lt;pred&gt;
               &lt;rb base="never"&gt;never&lt;/rb&gt;
               &lt;vb base="do"&gt;did&lt;/vb&gt;
               &lt;vb base="run"&gt;run&lt;/vb&gt;
               &lt;/pred&gt;
               &lt;jj base="smooth"&gt;smooth&lt;/jj&gt;
               &lt;dl base="."&gt;.&lt;/dl&gt;
               &lt;/s&gt;
               
               
            </div>
            <div class="para e135">Many current text analytics systems use a lexicon, part of speech tagging, and shallow
               parsing to extract such linguistic structures. While far from a complete analysis
               of linguistic structure, these features permit much more sophisticated evaluation
               of "what's going on" than strictly word-level features such as described earlier.
               For example, knowing whether a topic showed up as a subject vs. an object or a mere
               modifier, is invaluable for estimating it's importance. Knowing whether a given action
               is past or future, part of a question, or subject to connectives such as "if" or "but"
               (not to mention negation!) also has obvious value. Having a handle on sentence structure
               also helps reveal what is said 
               <div class="emphasis e136" role="ital">about</div> a given topic, when a topic is referred to indirectly (by pronouns, generic labels
               like "the actor", etc.).
            </div>
         </div><a name="Accuracy?ANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="section e137">
            <div class="title e138">Accuracy?</div>
            <div class="para e139">Users of text analytics systems always ask "how accurate is it?" Unlike XML validation,
               this is not a simple yes/no question, and so using TA in creating or checking markup
               is a probabilistic matter. As it turns out, even a "percent correct" measure is often
               a misleading oversimplification. In the interest of brevity, I'll give a few examples
               of the problems with a unitary notion of "accuracy", using Sentiment as an example
               (since it is perhaps the most common TA measure in practice):
            </div>
            <div class="itemizedlist e140">
               <div class="listitem e141">
                  <div class="para e142">Let's say a system categorizes texts as having "Positive" or "Negative" sentiment
                     (leaving aside the precise definition of "Sentiment"), and gets the "right" answer
                     for 70% of the test documents. The first key question is how the desired answer came
                     to be considered "right" in the first place. Normally, accuracy is measured against
                     human ratings on a set of texts. Yet if one asks several people to rate a particular
                     text, they only agree with each other about 70% of the time
                     <div class="popupBox e143">
                        <div class="popupLabel" onmouseover="$('#d1e257').show('1000');" onmouseout="$('#d1e257').hide('1000');">[ footnote ]</div>
                        <div id="d1e257" style="display: none;">
                           <div class="footnote">
                              <div class="para e144">The Kohen's Kappa statistic is a commonly-used measure of Inter-Rate Reliability.</div>
                           </div>
                        </div>
                     </div>. If texts only have one rater, 30% of the texts probably have debatable ratings.
                     If one throws out all the cases where people disagree, that unfairly inflates the
                     computer's score because all the hard/borderline cases are gone. Treating the humans'
                     ratings like votes, if the computer agrees with the consensus of 3 humans 70% of the
                     time, is it 70% accurate, or 100% as good as a human? If the algorithm does even better,
                     say 80%, what does that even mean? In considering applications to XML, it would be
                     interesting to know how closely human readers agree about the matters TA might be
                     called on to evaluate.
                  </div>
               </div>
               <div class="listitem e145">
                  <div class="para e146">In practice, Sentiment requires a third category: "Neutral". A corpus that is 80%
                     neutral, 5% negative, and 15% positive is typical. That means a program can beat the
                     last example merely by always returning "Neutral": that's 80% accurate, right? This
                     illustrates the crucial distinction of 
                     <div class="emphasis e147" role="ital">precision</div> versus 
                     <div class="emphasis e148" role="ital">recall</div>: this strategy perfectly recalls (finds) 100% of the neutral cases; but it's not
                     very precise: 1/5 of the texts it calls "Neutral" are wrong. In addition, it has 0%
                     recall for positives and negatives (which are much more important to find for most
                     purposes).
                  </div>
               </div>
               <div class="listitem e149">
                  <div class="para e150">At some level Sentiment is typically calculated as a real number, not just three categories;
                     say, ranging from -1.0 to +1.0. How close to 0.0 counts as Neutral? That decision
                     is trivial to adjust, and may make a huge difference in the precision and recall (and
                     note that the Neutral zone doesn't have to be symmetrical around 0).
                  </div>
               </div>
               <div class="listitem e151">
                  <div class="para e152">For systems that generate quantities instead of just categories, should a near miss
                     be considered the same as a totally wild result? Depending on the user's goals and
                     capabilities, a close answer might (or might not) still be useful. Using TA methos
                     to evaluate XML document portions is a likely case of this: Although most abstracts
                     are likely to be broadly positive, only severe negativity would likely warrant a warning;
                     similarly, so long as an abstract is reasonably stylistically "abstract-ish", it's
                     probably ok (but not if it looks "bibliography-ish").
                  </div>
               </div>
               <div class="listitem e153">
                  <div class="para e154">Many documents longer than a treet have a mixture of positive, neutral, and negative
                     sentiments expressed toward their topics. Does a long document that's scrupulously
                     neutral, deserve the same sentiment as one that describes two very strong opposing
                     views? They may average out the same, but that's not very revealing.
                  </div>
               </div>
               <div class="listitem e155">
                  <div class="para e156">A subtler problem is that rating a document's sentiment toward a given topic depends
                     on rightly identifying the topic. What if the topic isn't identified right? Is "Ms.
                     Gaga" the same as "Lady Gaga"? Is the topic economics, the national debt, or fiscal
                     policy? Some systems avoid this problem by reporting only an "overall" sentiment instead
                     of sentiment towards each specific topic, but that greatly exacerbates the previous
                     problem.
                  </div>
               </div>
               <div class="listitem e157">
                  <div class="para e158">Users' goals may dictate very different notions of what counts as positive and negative.
                     For market analysts or reporters, a big change in a company's stock price may be "just
                     data": good or bad for the company, but just a fact to the analyst. Political debate
                     obviously has similar issues.
                  </div>
               </div>
            </div>
            <div class="para e159">Understanding those general methods and caveats, it's fair to generalize that text
               analysis systems typically detect features with 70-80% accuracy, although some features
               are far easier than others. Language identification is far more accurate; sarcasm
               detection, far less. This means that such systems work best when there are many texts
               to process -- the errors will generally come out in the statistical wash.
            </div>
         </div>
      </div><a name="TextAnalytics'RelationtoMarkupANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="section e160">
         <div class="title e161">Text Analytics' Relation to Markup</div>
         <div class="para e162">Markup has multiple purposes; among them are</div>
         <div class="itemizedlist e163">
            <div class="listitem e164">
               <div class="para e165">Disambiguating structure (e.g., famous OED italics)</div>
            </div>
            <div class="listitem e166">
               <div class="para e167">Controlling layout and other processing</div>
            </div>
            <div class="listitem e168">
               <div class="para e169">Identifying things to search on</div>
            </div>
         </div>
         <div class="para e170">Markup makes aspects of document structure explicit. In principle, any phenomenon
            that text analytics can identify, can then be marked up, to a corresponding level
            of accuracy. Exactly the same analytics can be used in checking: If a text is already
            marked up for feature X, we need only run an auto-tagger for X and compare. This simultaneously
            gives feedback on the text-analytic output's accuracy, and the prior markup's.
         </div>
         <div class="para e171">When two sources of data are available like this, they can be used to check each other.
            In addition, the degree of overlap in what is "found" by each source, enables estimating
            the number of cases not found be either. A simple statistic called the "Lincoln Index",
            originating in species population surveys, provides this estimate. In the same way,
            text analytics can be used to do XML markup de novo, or as a direct quality check
            on existing markup. 
         </div>
         <div class="para e172">Such comparative analysis may be one of the most useful applications of TA to XML
            evaluation. In a text project where markup is not straightforward, how can one evaluate
            how well the taggers are doing? Say a literary project is marking up subjective features
            such as allusions, or sometimes-unclear features such as who the speaker is for each
            beat in dialog. TA methods can learn how to characterize such features, and then be
            run against the human-tagged texts. Disagreement can trigger a re-check, thus saving
            time versus checking everything.
         </div>
         <div class="para e173">There seems to be an implicit "sweet spot" for markup use. We don't mark up sufficiently
            obvious phenomena, such as word boundaries (except in special cases). Given that almost
            every kind of processing needs to know where they are, why not? Probably because finding
            word boundaries seems trivial in English.
            <div class="popupBox e174">
               <div class="popupLabel" onmouseover="$('#d1e311').show('1000');" onmouseout="$('#d1e311').hide('1000');">[ footnote ]</div>
               <div id="d1e311" style="display: none;">
                  <div class="footnote">
                     <div class="para e175">It isn't quite trivial; there are many edge cases such as "$10 million", "AT&amp;T", "@twitter",
                        ":)", H
                        <div class="subscript e176">2</div>O, "New York-based", contractions, and some particularly ugly cases where people use
                        hyphen when they mean emdash. Humans don't usually need a truly precise notion of
                        "word", and our writing systems don't provide one.
                     </div>
                  </div>
               </div>
            </div> Yet word boundaries are also unlikely to be marked up in Asian languages, where identifying
            them is far from trivial. Thus, simplicity can't be the whole story. Perhaps it is
            that consciously or not we assume that most any downstream software will do this by
            itself, so there would be no return on even a small investment in explicit markup.
         </div>
         <div class="para e177">Language-use has better ROI, for example enabling language-specific spell-checking
            or indexing. Downstream software is perhaps less likely to "just handle it." Nevertheless,
            it is not very common to see 
            <div class="code e178">xml:lang</div> more than once in a document except in special cases such as bilingual dictionaries,
            diglot literature editions, and the like.
         </div>
         <div class="para e179">TA systems can certainly add (or check) word-boundary and language-use markup, and
            the most common related attributes, such as part of speech and lemma. Such markup
            is perhaps of limited value except in special applications, such as text projects
            in Classical languages or that contend with paleographic issues.
         </div>
         <div class="para e180">Marking up small-scope, very specific semantics such as emphasis and the role of particular
            nouns is a traditionally awkward matter in markup. Some schemas merely provide tags
            for italic, bold, and the like; using less font-oriented tags such as &lt;emph&gt; is considered
            a step up, but often accomplished little more than moving "italic" from element to
            attribute. If more meaningful small-scale elements are not available, conventions
            such as RDF[
            <div class="xref e181" linkend="RDF"></div>] and microformats[
            <div class="xref e182" linkend="micro"></div>] make it feasible to represent the results of text analytics or even linguistic parsing
            in ways accessible to XML systems.
         </div>
         <div class="para e183">DocBook[
            <div class="xref e184" linkend="docb"></div>] provides many more meaningful items at this level: 
            <div class="code e185">guibutton, keycap, menuchoice</div>, etc. As with other fairly concrete features already described and as an anonymous
            referee pointed out, text analytics could be used to tag (or check) many such distinctions
            automatically: "ENTER" is going to be a 
            <div class="code e186">keycap</div>, not a 
            <div class="code e187">guiItem</div>; in other cases nearby text such as "press the ___ key" can help, as can context
            such as being in a section titled "List of commands". This seems entirely tractable
            for analytics, and could have significant value because such markup is valuable for
            downstream processings (particularly search), but tedious for humans to create or
            check, and therefore error-prone.
         </div>
         <div class="para e188">However, even this level of markup can get subtle. In 
            <div class="emphasis e189" role="ital">Making Hypermedia Work</div>[DeRo94] David Durand and I decided to distinguish SGML element names from HyTime
            architectural form names in the text, because the distinction is crucial but, at least
            to the new user, subtle. We also decided, in many examples, to name elements the same
            as the architectural form they represented. In most cases there was only one element
            of a given form under discussion; and because elements are concrete while forms are
            abstract, one cannot easily reify the latter without the former. In most cases this
            was trivial; but in a few cases the decision seemed impossible. Examing those cases
            via TA methods would likely reveal much about the distinction we were trying to achieve,
            as well as no doubt reveal marup errors.
         </div>
         <div class="para e190">Bibliography entries are notoriously troublesome, whether in XML or any other representation.
            They have many sub-parts, with complex rules for which parts can co-occur; the order
            (not to mention formatting) of the parts varies considerably from one publisher to
            another; and there are special cases that are difficult given the usual rules. PubMedCentral
            receives an extraordinary number of XML articles, often in a standard XML Schema[NCBI];
            but usage varies significantly even in valid data, and the code to manage bibliographic
            entries in the face of such variability is substantial. Many publishers opt for the
            "free" style, in which most schema constraints are relaxed, and recovering the meaningful
            parts of entries is a task worthy of AI.
         </div>
         <div class="para e191">At a higher or at least larger level, many schemas are heavy on tags for idiosyncratic
            components with much linguistic content, which also have distinctive textual features:
            Bibliography, Abstract, Preface, etc. For example, a Preface will likely use much
            future tense, while a Prior Work section will use past. Text analytics can find and
            quantify such patterns, and then find and report outliers, which might show up due
            to tagging errors, or to writing which, whether through error or wisdom, does not
            fit the usual patterns.
         </div>
         <div class="para e192">This provides a particularly promising area for applying text analytics. Although
            the titles of such sections differ, and there may or not be distinct tags for each
            kind, a text analytics system could learn the distinctive form of such components,
            and then evaluate how consistent the tagging and/or content of corresponding sections
            in other documents are.
         </div>
         <div class="para e193">We usually mark up things that are necessary for layout; the ROI is often obvious
            and quick. But it takes a lot of dedication, sophistication, and money to, say, disambiguate
            the many distinct uses of italics, bold, and other typography in the Oxford English
            Dictionary[Tom91], or to characterize the implicitly-expected style for major components
            such as those in front and back matter.di. Many of the implicit data described earlier
            can be detected using text analytic methods, but using this to assist the markup process
            has been little explored.
         </div>
      </div><a name="Howshallwefindtheconcordofthisdiscord?ANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="section e194">
         <div class="title e195">How shall we find the concord of this discord?</div>
         <div class="para e196">If you can find it reliably via some algorithm, why mark it up? In a sense, creating
            markup via algorithms is kind of like the old saw about Artificial Intelligence: "as
            soon as you know how to do it, it's not AI anymore." If text analytics (or any other
            technology) could completely reliably detect some feature we used to mark up, we might
            stop marking it up. But in reality, neither humans nor algorithms are entirely reliable
            for marking up items of interest. The probability that the two will err in quite different
            ways, means there is synergy to be had.
         </div>
         <div class="para e197">Anyone who has tried to write a program to turn punctuated quotes into marked-up quote
            elements, has discovered that there are many difficult cases, at a variety of levels.
            Choice of character set and encoding, international differences in style, nested and/or
            continued quotations in dialog, alternate uses of apostrophe and double quote, and
            even quotations whose scope crosses tree boundaries[see 
            <div class="xref e198" linkend="DeRo04"></div>]. Would we bother marking quotations if the punctuation were unambiguous, or if we
            had widespread text-analytics solutions that could always identify quotations for
            formatting search, and other common needs?
         </div>
         <div class="para e199">Typical XML schemas define some very common specific items: Bibliography, Table of
            Contents, Preface; and some common generic items: Chapter, Appendix, etc. But (perhaps
            pragmatically) we don't enumerate the many score front and back matter sections listed
            in the Manual of Style, or the additional ones that show up in countless special cases
            -- at some point we just say "front-matter section, type=NAME" and quit. Worse, we
            sometimes cannot choose the "correct" markup: whether we are the original author or
            a later redactor, we may simply be unable to say whether to mark up "Elizabeth" as
            &lt;paramour&gt; or &lt;tourist&gt; in "Elizabeth went to Essex. She had always liked Essex."[
            <div class="xref e200" linkend="teip5"></div>]
         </div>
         <div class="para e201">The short response to these issues, I think, is that markup is always a tradeoff;
            there are levels we make explicit, and levels we don't. Perhaps it cannot be otherwise.
            Intuitively, it seems that at least for authors many of the choices should always
            be clear; and to that extent text analytics can also find many of these phenomena.
            So why does the principle not work, that an author knows when component X is needed,
            and so should have an easier time just naming X, than carrying out commands to achieve
            a look that others will (hopefully) process back to X?[
            <div class="xref e202" linkend="Coom87"></div>]
         </div>
         <div class="para e203">I think it is because people's interaction with language (not 
            <div class="emphasis e204" role="ital">via</div> language) is largely unconscious. We rarely think "the next preposition is important,
            so I'm going to say it louder and slower"; we don't think "I've said everything that
            relates to that topic sentence, so it's time for a new paragraph"; nor even "'Dr'
            is an abbreviation, so I need a period". Expertise has been defined as the reduction
            of more and more actions to unconsciousness -- that's how we (almost always) walk
            and/or chew gum. Our understanding of language is often similarly tacit. As Dreyfuss
            and Dreyfuss put it[
            <div class="xref e205" linkend="Drey05"></div>, p. 788), 
            <div class="quote e206">If one asks an expert for the rules he or she is using, one will, in effect, force
               the expert to regress to the level of a beginner and state the rules learned in school.
               Thus, instead of using rules he or she no longer remembers, as the knowledge engineers
               suppose, the expert is forced to remember rules he or she no longer uses.
            </div>
         </div>
         <div class="para e207">The act of markup, whether automated or manual, seems similar: we know a paragraph
            (or emphasis, or lacuna) when we see it, just as we know an obstacle on the sidewalk
            when we see it; but neither often makes it to consciousness.
         </div>
         <div class="para e208">Text analytics and markup are very similar tasks, though they tend to identify different
            things; it is rare for (say) a literary text project to mark up sentiment in novels,
            while it is equally rare for text anaytics to identify emphasis (although emphasis
            mught contribute to other features, such as topic weight).
         </div>
         <div class="para e209">Perhaps the most obvious place to start, beyond simple things like language-identification,
            is checking whether existing markup "makes sense", at a higher level of abstraction
            that XML schema languages -- a level closely involving the language rather than the
            text. The usual XML schema languages do little or nothing with text-content; with
            DTDs one can strip out *all* text content and the validity status of a document cannot
            change. With a text analytics system in place, however, it is possible to run tests
            related to the actual meaning of content. For example:
         </div>
         <div class="itemizedlist e210">
            <div class="listitem e211">
               <div class="para e212">After finding the topics of each paragraph, one can estimate the cohesiveness of sections
                  and chapters, as well as check that section titles at least mention the most relevant
                  topics. Comparison between the topics found in an abstract, and the topics found in
                  the remainder of an article, could be quite revealing.
               </div>
            </div>
            <div class="listitem e213">
               <div class="para e214">The style for a technical journal might require a Conclusions section (which might
                  want to be very topically similar to the abstract), and a future work section that
                  should be written in the future tense. Similarly, a Methods section should probably
                  come out low on subjectivity. In fiction, measuring stylistic features of each character's
                  speech could reveal either mis-attributed speeches, or inconsistencies in how a given
                  character is presented.
               </div>
            </div>
            <div class="listitem e215">
               <div class="para e216">The distribution of specific topics can also be valuable: Perhaps a definition should
                  accompany the *first* use of a given topic -- this is relatively easy to check, and
                  a good text analytic system will not often be fooled by a prior reference not being
                  identical (for example, plural vs. singular references), or by similar phrases that
                  don't actually involve the same topic.
               </div>
            </div>
            <div class="listitem e217">
               <div class="para e218">One important task in text analytics is identification of "Named Entities": Is this
                  name a person, organization, location, etc? Many XML schemas are rich in elements
                  whose content should be certain kinds of named entities: &lt;author&gt;, &lt;editor&gt;, &lt;copyright-holder&gt;,
                  &lt;person&gt;, &lt;place&gt;, and many more. These can easily be checked by many TA systems.
                  Since TA systems typically use large catalogs of entities, marked-up texts can also
                  contribute to TA systems as entity sources.
               </div>
            </div>
         </div>
         <div class="para e219">Text analytics is strongest at identifying abstract/conceptual features, when those
            features are not easily characterized by specific words or phrases, but emerge from
            larger linguistic context. The most blatant example is the perennial problem with
            non-language-aware search engines: negation. There are many ways to invert the sense
            (or sentiment) of a text, some simple buy many extremely subtle or complex. Tools
            that do not analyze syntax, clause roles, and the like can't distinguish texts that
            mean nearly the opposite of each other. Thus, at all levels from initial composition
            and markup, through validation and production, to search and retrieval, text analytics
            can enable new kinds of processes. Perhaps as such technology becomes widespread and
            is integrated into our daily workflows, it may help us to reach more deeply into the
            content of our texts.
         </div>
         <div class="para e220">Little has been published on the use of text analytics in direct relation to markup,
            although text analytics tools often use XML extensively, particularly for the representation
            of their results. However, TA has the potential to contribute significantly to our
            ability to validate exactly those aspects of documents, that markup does not help
            with: namely, what's going on down in the leaves.
         </div>
      </div><a name="BibliographyANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="bibliography e221">
         <div class="title e222">Bibliography</div>
         <div class="bibliomixed e223" xml:id="Abr63" xreflabel="Abr63">Norman Abramson. 1963. Information Theory and Coding. New York: McGraw-Hill.
            
         </div>
         <div class="bibliomixed e224" xml:id="Coom87" xreflabel="Coom87">James H. Coombs, Allen H. Renear, and Steven J. DeRose. 1987. "Markup systems and
            the future of scholarly text processing." Communications of the ACM 30, 11 (November
            1987), 933-947. doi:
            <div class="biblioid doi e225">10.1145/32206.32209</div>.
            	
         </div>
         <div class="bibliomixed e226" xml:id="DeRo04" xreflabel="DeRo04">Steven DeRose. 2004. "Markup Overlap: A Review and a Horse."ï¿¼Extreme Markup Languages
            2004, MontreÌal, QueÌbec August 2-6, 2004. 
            <div xmlns:xlink="http://www.w3.org/1999/xlink" class="link e227" xlink:actuate="onRequest" xlink:show="new" xlink:type="simple">http://xml.coverpages.org/DeRoseEML2004.pdf</div>
            	
         </div>
         <div class="bibliomixed e228" xml:id="DeRo94" xreflabel="DeRo94">Steven DeRose and David Durand. 1994. "Making Hypermedia Work: A User's Guide to HyTime."ï¿¼Boston:
            Kluwer Academic Publishers.
            	
         </div>
         <div class="bibliomixed e229" xml:id="Drey05" xreflabel="Drey05">Hubert L. Dreyfus and Stuart E. Dreyfus.
            "Peripheral Vision: Expertise in Real World Contexts." Organization studies 26(5):
            779-792. doi:
            <div class="biblioid doi e230">10.1177/0170840605053102</div>.
            
         </div>
         <div class="bibliomixed e231" xml:id="Gate" xreflabel="Gate">Gate: General Architecure for Text Engineering 
            <div xmlns:xlink="http://www.w3.org/1999/xlink" class="link e232" xlink:actuate="onRequest" xlink:show="new" xlink:type="simple">http://gate.ac.uk</div>
            
         </div>
         <div class="bibliomixed e233" xml:id="het1" xreflabel="het1">"Heteronym Homepage" 
            <div xmlns:xlink="http://www.w3.org/1999/xlink" class="link e234" xlink:actuate="onRequest" xlink:show="new" xlink:type="simple">http://www-personal.umich.edu/~cellis/heteronym.html</div>
            	
         </div>
         <div class="bibliomixed e235" xml:id="het2" xreflabel="het2">"The Heteronym Page" 
            <div xmlns:xlink="http://www.w3.org/1999/xlink" class="link e236" xlink:actuate="onRequest" xlink:show="new" xlink:type="simple">http://jonv.flystrip.com/heteronym/heteronym.htm</div>
            	
         </div>
         <div class="bibliomixed e237" xml:id="Kla96" xreflabel="Kla96">Judith Klavans and Philip Resnik. The Balancing Act: Combining Symbolic and Statistical
            Approaches to Language. MIT Press 1996. 978-0-262-61122-0.
            	
         </div>
         <div class="bibliomixed e238" xml:id="micro" xreflabel="micro">Microformats (home page) 
            <div xmlns:xlink="http://www.w3.org/1999/xlink" class="link e239" xlink:actuate="onRequest" xlink:show="new" xlink:type="simple">http://microformats.org</div>
            
         </div>
         <div class="bibliomixed e240" xml:id="Mont73" xreflabel="Mont73">Richard Montague. 1973. "The Proper Treatment of Quantification in Ordinary English".
            In: Jaakko Hintikka, Julius Moravcsik, Patrick Suppes (eds.): Approaches to Natural
            Language. Dordrecht: 221â242. doi:
            <div class="biblioid doi e241">10.1007/978-94-010-2506-5_10</div>.
            	
         </div>
         <div class="bibliomixed e242" xml:id="NCBI" xreflabel="NCBI">National Center for Biotechnology Information, National Library of Medicine, National
            Institutes for Health. "Journal Publishing Tag Set".
            	
         </div>
         <div class="bibliomixed e243" xml:id="NLTK" xreflabel="NLTK">NLTK 2.0 documentation: The Natural Language Toolkit. 
            <div xmlns:xlink="http://www.w3.org/1999/xlink" class="link e244" xlink:actuate="onRequest" xlink:show="new" xlink:type="simple">http://www.nltk.org</div>
            
         </div>
         <div class="bibliomixed e245" xml:id="docb" xreflabel="">OASIS Docbook TC. 
            <div xmlns:xlink="http://www.w3.org/1999/xlink" class="link e246" xlink:actuate="onRequest" xlink:show="new" xlink:type="simple">http://www.oasis-open.org/committees/tc_home.php?wg_abbrev=docbook</div>
            	
         </div>
         <div class="bibliomixed e247" xml:id="oa" xreflabel="">OpenAmplify. 
            <div xmlns:xlink="http://www.w3.org/1999/xlink" class="link e248" xlink:actuate="onRequest" xlink:show="new" xlink:type="simple">http://www.openamplify.com</div>
            	
         </div>
         <div class="bibliomixed e249" xml:id="RDF" xreflabel="RDF">Resource Description Format. 
            <div xmlns:xlink="http://www.w3.org/1999/xlink" class="link e250" xlink:actuate="onRequest" xlink:show="new" xlink:type="simple">http://www.w3.org/RDF/</div>
            
         </div>
         <div class="bibliomixed e251" xml:id="Stede" xreflabel="Stede">Manfred Stede and Arhit Suriyawongkul. "Identifying Logical Structure and Content
            Structure in Loosely-Structured Documents." In Linguistic Modeling of Information
            and Markup Languages: Contributions to Language Technology. Andreas Witt and Dieter
            Metzing, eds., pp. 81-96.
            	
         </div>
         <div class="bibliomixed e252" xml:id="teip5" xreflabel="TEI P3">TEI Guidelines for the Encoding of Machine-Readable Texts. Edition P5.
            	
         </div>
         <div class="bibliomixed e253" xml:id="Tom91" xreflabel="Tom91">Frank Wm. Tompa and Darrell R. Raymond. 1991. "Database Design for a Dynamic Dictionary."
            In (Eds.) Susan Hockey and Nancy Ide. Research in Humanities Computing: Selected Paper
            from ALLC/ACH Conference, Toronto.
            	
         </div>
         <div class="bibliomixed e254" xml:id="WEKA" xreflabel="WEKA">Machine Learning Group at University of Waikato. "Weka 3: Data Mining Software in
            Java." 
            <div xmlns:xlink="http://www.w3.org/1999/xlink" class="link e255" xlink:actuate="onRequest" xlink:show="new" xlink:type="simple">http://www.cs.waikato.ac.nz/ml/weka/</div>. 
            	
         </div>
      </div>
   </div>
</div>