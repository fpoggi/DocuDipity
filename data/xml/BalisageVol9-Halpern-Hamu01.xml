<?xml version="1.0" encoding="UTF-8"?><article xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0-subset Balisage-1.3"><title>Case study: Quality assurance and quality control techniques in an XML data conversion project
  </title><info><confgroup><conftitle>International Symposium on Quality Assurance and Quality Control in XML</conftitle><confdates>August 6, 2012</confdates></confgroup><abstract><para>
        A wide variety of techniques have been used in an XML data conversion project.
        Emphasis on Quality Assurance, not making errors in the first place,
        was supported by Quality Control, catching errors that occurred anyway.
      </para><para>
        Data analysis and estimation techniques included
        counting function points in source documents to estimate effort
        and autogeneration of tight schemas to discover variation.
        Quality assurance was based on guiding specification based
        on parent-child pairs and programming for context and all content.
        Quality Control techniques included source-to-target comparison to check for lost or duplicated content,
        automatic highlighting of anomalous data, and use of XQuery to review data.
      </para></abstract><author><personname><firstname>Charlie</firstname><surname>Halpern-Hamu</surname></personname><personblurb><para>
          Charlie has been working with structured text since 1991.
          During this time, he has acted as a content and systems architect, programmer, systems integrator, consultant,
          mentor, best-practices coordinator, trainer, book editor, project lead, department manager, and vice president.
          His consulting and training work has taken him all over North America
          as well as visits to South America, Europe, Australia and China.
          Charlie has a PhD in Computer Science from the University of Toronto and an MBA from Heriot-Watt University.
          He's good at making complex systems easy to understand. Or so he claims.
        </para></personblurb><affiliation><jobtitle>Senior Solutions Architect</jobtitle><orgname>Tata Consultancy Services</orgname></affiliation><email>charlie.hamu@tcs.com</email></author><legalnotice><para>Â© Copyright 2012, Tata Consultancy Services.</para><para>Disclaimer: All views expressed in the publication are of the author and Tata Consultancy Services (TCS) does not warrant, either expressly or implied, the accuracy, appropriateness of the information in the publication. TCS disclaims any responsibility for content error, omissions and any responsibility associated with relying on the information provided in the publication.</para></legalnotice><keywordset role="author"><!-- my list --><keyword>function points</keyword><keyword>elements in context</keyword><keyword>parent/child pairs</keyword><keyword>estimating</keyword><keyword>converting</keyword><keyword>conversion slope</keyword><keyword>autogenerated schemas</keyword><keyword>programming best practices</keyword><keyword>automated testing</keyword><!-- from Balisage concept list --><keyword>programming</keyword><keyword>quality</keyword><!-- from Balisage specifiations list --><keyword>XQuery</keyword><keyword>XSLT</keyword><!-- from Balisage list --><keyword>querying</keyword><keyword>validating</keyword></keywordset></info><section><title>
      Context and Goals
    </title><para>
      A recently completed data conversion project
      provided an opportunity to combine several different techniques
      for XML quality assurance and quality control.
    </para><para>
      The data conversion project was a significant cost and significant risk for the company.
      Data in two languages, and several hundred SGML DTDs, was converted to a single XML DTD.
      The company was a legal publisher, and this data was the company's key money-making asset.
      The conversion project stretched over multiple years and multiple teams in multiple locations.
    </para><para>
      The time and money involved meant that it was crucial to specify clearly what was to be done,
      and to be able to estimate as accurately as possible the amount of effort involved to complete each subproject.
      It was critical that no data be lost or damaged,
      and that any errors be caught before the converted data went into production.
    </para><para>
      The nine techniques discussed in this paper can be grouped into three broad categories:
      <orderedlist><listitem><para>
            Data Analysis and Estimation:
            <itemizedlist><listitem><para>
                  Counting function points in source to estimate effort.
                </para></listitem><listitem><para>
                  Count function points in target to estimate slope.
                </para></listitem><listitem><para>
                  Autogenerate tight schemas to discover variation.
                </para></listitem></itemizedlist>
          </para></listitem><listitem><para>
            Quality Assurance:
            <itemizedlist><listitem><para>
                  List parent-child pairs to guide specification.
                </para></listitem><listitem><para>
                  Always program for context.
                </para></listitem><listitem><para>
                  Always program for all content.
                </para></listitem></itemizedlist>
          </para></listitem><listitem><para>
            Quality Control:
            <itemizedlist><listitem><para>
                  Compare source to target for lost or duplicated content.
                </para></listitem><listitem><para>
                  Autogenerate word wheels to highlight anomalous data.
                </para></listitem><listitem><para>
                  Use XQuery-capable XML database to quickly review data.
                </para></listitem></itemizedlist>
          </para></listitem></orderedlist>
    </para></section><section><title>
      Data Analysis and Estimation
    </title><para>
      The first steps to a high-quality conversion project
      include getting a clear sense of the size of the job,
      the complexity of the conversion, and variability of the data.
      Though hardly complete,
      the techniques listed in this section address each of these concerns.
    </para><section><title>
        Count function points in source to estimate effort
      </title><para>
        The basic metric we use to estimate the size of a conversion project or subproject is to count the
        <quote>conversion function points</quote>
        in the input data, defined as follows:
      </para><orderedlist><listitem><para>
            Each parent/child pair counts for one.
            So, for example
            <code>article/para/bold</code>
            and
            <code>section/para/bold</code>
            count for a single point, but
            <code>section/title/bold</code>
            merits a second point.
          </para></listitem><listitem><para>
            Each element/attribute pair counts for one.
            So
            <code>tr/@align</code>
            and
            <code>td/@align</code>
            count as two function points.
          </para></listitem><listitem><para>
            Text, processing instructions, and comments are ignored.
            This is based on the observation/assumption that most conversions
            pass these kinds of information through without complicated logic.
          </para></listitem></orderedlist><para>
        The function point count allows us to estimate the programming effort.
        In our experience, as a rule-of-thumb starting point,
        specification, programming and QC come out to about an hour per function point.
        Already-specified function points can usually be deducted from estimate.
      </para><para>
        Obviously, other metrics could be used.
        In determining how much context should count,
        experience suggested that no context (i.e. all
        <code>bold</code>
        considered the same) seemed too little,
        multiple levels of context (i.e.
        <code>article/para/bold</code>
        and
        <code>section/para/bold</code>
        considered different) seemed too much,
        and one level of context seemed about the right balance.
        By
        <quote>right balance</quote>
        we mean that the resulting function point count varied approximately with the programming effort required.
      </para><para>Benefit: This metric is objective, transparent, and repeatable.</para></section><section><title>Count function points in target to estimate slope</title><para>
        When available, an estimation of the function points in the target
        allows us to estimate the slope, or difficulty, of the conversion.
      </para><para>
        Example:
        <itemizedlist><listitem><para>Input sample = 72 conversion function points</para></listitem><listitem><para>Corresponding output = 101 conversion function points</para></listitem><listitem><para>101 / 72 = 40% bulk up</para></listitem></itemizedlist>
      </para><para>
        Note that conversion effort is sometimes more closely related to the number of output markup combinations
        that must be produced than to the number of input markup combinations.
        For example, it's much easier to convert both
        <code>foreign</code>
        and
        <code>pub-title</code>
        to
        <code>italic</code>
        than it is to map
        <code>italic</code>
        to either
        <code>foreign</code>
        or
        <code>pub-title</code>
        depending on other clues.
        So, with one-to-one (slope = 1) as the baseline, greater bulk-up factors
        typically represent greater complexity, difficulty and effort in the conversion.
      </para><para>
        But usually, by the time output is available for analysis, the conversion is done.
        As target markup is often not available,
        slope is more commonly estimated by
        the number of text-pattern-to-element rows in the specification.
        Or, a smaller sample is used to estimate slope,
        and then this slope is assumed to apply to the fuller data set.
      </para><para>
        Recommendation: during
        <emphasis role="ital">conversion</emphasis>
        keep slope modest.
        Slopes much higher than level can be defined to be data enhancements,
        rather than data conversions.
        As such, they may be more successfully undertaken once the basic conversion is complete.
        This is especially true in contexts such as the present case study,
        where the initial conversion collapsed hundreds of sometimes contradictory SGML DTDs
        to a single consistent XML DTD.
      </para></section><section><title>
        Autogenerate tight schemas to discover variation
      </title><para>
        Using free tools such as inst2xsd,
        any number of XML files can be used to create a schema.
        <citation linkend="inst2xsd">[inst2xsd]</citation>
      </para><para>
        A schema can be generated for an initial set of files,
        and then this schema can be used to validate additional files.
        The resulting validation errors indicate new input variations that must be accomodated.
      </para><para>
        This same technique can equally well be applied to conversion outputs.
        New patterns in the outputs can indicate where downstream processes may need to be extended,
        or may be indications of a conversion process that's gone off the rails.
      </para><para>
        Benefit: Auto-generated tight schemas are one way to highlight variation in the data.
      </para></section></section><section><title>
      Quality Assurance
    </title><para>
      Our goal in quality assurance is to avoid introducing errors in the first place.
      Our techiques are a collection of best practices for specification and programming,
      three of which are highlighted here.
    </para><section><title>List parent-child pairs to guide specification</title><para>
        Creating a simple list of all parent/child pairs in the input
        can be used to create a simple framework for specification.
        <informaltable><tr><th>Input</th><th>Context</th><th>Output</th><th>Notes</th></tr><tr><td>
              <code>@align</code>
            </td><td>
              <code>colspec</code>
            </td><td>to-do</td><td>to-do</td></tr><tr><td>
              <code>@align</code>
            </td><td>
              <code>entry</code>
            </td><td>to-do</td><td>to-do</td></tr><tr><td>
              <code>b</code>
            </td><td>
              <code>entry</code>
            </td><td>to-do</td><td>to-do</td></tr><tr><td>
              <code>b</code>
            </td><td>
              <code>paragraph</code>
            </td><td>to-do</td><td>to-do</td></tr></informaltable>
      </para><para>
        These parent/child pairs are the same that we used as the definition of
        <quote>conversion function point</quote>
        for estimation purposes above, and so we already have generated these lists.
      </para><para>
        This same list can provide a simple starting skeleton for the conversion script:
      </para><programlisting xml:space="preserve">
&lt;template match="colspec/@align | entry/@align"&gt;
 &lt;call-template name="to-do"/&gt;
&lt;/template&gt;
&lt;template match="entry/b | paragraph/b"&gt;
 &lt;call-template name="to-do"/&gt;
&lt;/template&gt;</programlisting></section><section><title>
        Always program for context
      </title><para>
        Mapping without context is deceptively fast:
      </para><programlisting xml:space="preserve">
&lt;template match="<emphasis role="bold">b</emphasis>"&gt;
 &lt;call-template name="<emphasis role="bold">map-to-bold</emphasis>"/&gt;
&lt;/template&gt;</programlisting><para>
        But new contexts often require new consideration:
      </para><programlisting xml:space="preserve">
&lt;template match="<emphasis role="bold">entry</emphasis>/b"&gt;
 &lt;call-template name="<emphasis role="bold">map-to-heading-cell</emphasis>"/&gt;
&lt;/template&gt;
&lt;template match="<emphasis role="bold">paragraph</emphasis>/b"&gt;
 &lt;call-template name="<emphasis role="bold">map-to-bold</emphasis>"/&gt;
&lt;/template&gt;</programlisting><para>
        Benefit: Defensive programming avoids accidents before they happen.
      </para></section><section><title>Always program for all content</title><para>
        We may script a paragraph by writing code for each of the following attributes:
      </para><itemizedlist><listitem><para>
            <code>paragraph/@font_size</code>
          </para></listitem><listitem><para>
            <code>paragraph/@keep-next</code>
          </para></listitem><listitem><para>
            <code>paragraph/@keep-previous</code>
          </para></listitem><listitem><para>
            <code>paragraph/@leading</code>
          </para></listitem><listitem><para>
            <code>paragraph/@no-keeps</code>
          </para></listitem><listitem><para>
            <code>paragraph/@type</code>
          </para></listitem></itemizedlist><para>
        But if we do that, we will lose:
        <itemizedlist><listitem><para>
              <code>paragraph/@prespace</code>
            </para></listitem></itemizedlist>
      </para><para>
        ...unless we script for
        <quote>and all other attributes.</quote>
      </para><para>
        Benefit: Avoid silent loss of data.
      </para></section></section><section><title>
      Quality Control
    </title><para>
      Our goal in quality assurance was to avoid introducing errors.
      Our goal now, in quality control, is to reassure ourselves that we were successful,
      and to find the errors we no doubt made despite our best efforts.
    </para><section><title>
        Compare source to target for lost or duplicated content
      </title><para>
        The goal in comparing conversion input to conversion output is to note
        where we've lost or duplicated content.
        The basic concept is easy:
        delete all the
        <quote>markup</quote>
        and compare the
        <quote>content</quote>
        that remains.
        The execution is rather more difficult:
        <itemizedlist><listitem><para>
              Some input markup will become output content.
            </para></listitem><listitem><para>
              Some input content will become output markup.
            </para></listitem><listitem><para>
              Some input content will be puposely deleted.
            </para></listitem><listitem><para>
              Some input content will be purposely duplicated.
            </para></listitem></itemizedlist>
      </para><para>
        Nonetheless, we acheived good results by preprocessing both input and output files,
        rescuing some
        <quote>content</quote>
        from
        <quote>markup</quote>
        and duplicated and deleting content as required.
        Then we sort runs of text (another problem: determine what constitutes a run of text)
        in order to overcome issues with purposely reordered content.
        Finally, we use a side-by-side comparison tool
        to highlight mismatches between the massaged source and target.
      </para><para>
        Benefit: Catch lost and duplicated content.
      </para></section><section><title>
        Autogenerate of word-wheels to highlight anomalous data
      </title><para>
        I was first introduced to the concept of a
        <quote>word wheel</quote>
        in the context of
        <trademark>Folio Views</trademark>
        in the early 1990s.
        <citation linkend="Folio">[Folio]</citation>
        Folio Views was (and is) software for searching and browsing large sets of textual information -
        at the time, typically delivered on removable media.
        Folio Views for searching of predefined fields
        (in XML terms, typically metadata elements or semantic inline elements)
        across the data.
        One special feature, when searching fields, was autocompletion.
        When the users cursor was placed in the the search-form box corresponding to a particular field,
        all the values for that field were shown in an sorted list of unique values - called the
        <quote>word wheel.</quote>
        Just as with modern autocompletion, this list was updated as characters were typed in the search box.
      </para><para>
        The word wheel was a wonderful searching enhancement when it was introduced.
        But it could be a terrible embarrassment to the publisher if the source data was not clean.
        If searching for provinces, it was a wonderful help to be provided with a short list starting with Alberta and British Columbia.
        But if Alberta were mispelled anywhere in the thousands of documents being searched,
        it would show up in the short autocompletion list: Alberta, Alberto, British Columbia.
        Worse, if there were numerical values in a text field,
        they would sort to the top, making themselves all the more obvious.
      </para><para>
        The idea of adapting this word wheel behavior to become a quality control tool is simple.
        For each text-containing element in the converted output,
        with the exception of free-text fields like paragraphs,
        uniquely sort each of the observed values.
        Then ask a human to review the resulting sorted lists, looking for anomalies.
      </para><para>
        This approach very quickly highlights anomalous data, making it jump out to the human reviewer.
        At the same time, it avoids the wasted effort of reviewing thousands of redundantly correct values.
      </para><para>
        In the discussion that followed, Steve DeRose suggested that, in addition to an alphabetized list,
        a ranked list would give an additional insight.
        A ranked list would have the most common values at the top: not very interesting.
        And at the bottom, it would have the one-off values: possibly errors but also possibly one-off oddities.
        In the middle, you would have a sweet spot where you might find repeated errors.
      </para><para>
        Benefit: Makes human QC efficient.
      </para></section><section><title>Use an XQuery-capable XML database to quickly review data</title><para>
        When a suspicion arises, perhaps as a result of an odd word wheel entry
        or an unexplained mismatch between input and output content,
        it becomes very important that the offending data can be found quickly.
        And not only the specific example should be found,
        but we should review other places where similar behavior is likely to have occured.
        Having all input and all output instantly searchable using XQuery in invaluable.
        Given the availability of free and easy tools such as BaseX,
        there's really no excuse not to equip every team member with easy access to all the data.
        <citation linkend="BaseX">[BaseX]</citation>
      </para></section></section><section><title>
      Tangible Benefits
    </title><para>
      Consistent application of these techniques resulted in the following benefits to this project,
      as well as to other projects undertaken by Tata Consultancy Services:
      <itemizedlist><listitem><para>
            <emphasis role="bold">Objective, repeatable and reliable estimation</emphasis>
            from our conversion function point framework.
          </para></listitem><listitem><para>
            <emphasis role="bold">High quality</emphasis>
            results from programming best practices.
          </para></listitem><listitem><para>
            <emphasis role="bold">High productivity</emphasis>
            from reliable tools and techniques.
          </para></listitem><listitem><para>
            <emphasis role="bold">Scalability</emphasis>
            from systematic development approach.
          </para></listitem></itemizedlist>
    </para></section><bibliography><title>
      References
    </title><bibliomixed xml:id="BaseX">
      Wikipedia contributors,
      <quote>BaseX,</quote>
      <emphasis role="ital">Wikipedia, The Free Encyclopedia,</emphasis>
      <link xlink:href="http://en.wikipedia.org/wiki/BaseX" xlink:type="simple" xlink:show="new" xlink:actuate="onRequest">
        http://en.wikipedia.org/wiki/BaseX
      </link>
      (accessed 2012 July 13).
    </bibliomixed><bibliomixed xml:id="Folio">
      Wikipedia contributors,
      <quote>Folio Corporation,</quote>
      <emphasis role="ital">Wikipedia, The Free Encyclopedia,</emphasis>
      <link xlink:href="http://en.wikipedia.org/wiki/Folio_Corporation" xlink:type="simple" xlink:show="new" xlink:actuate="onRequest">
        http://en.wikipedia.org/wiki/Folio_Corporation
      </link>
      (accessed 2012 July 13).
    </bibliomixed><bibliomixed xml:id="inst2xsd">
      inst2xsd (Instance to Schema Tool),
      part of the Apache Project XMLBeans Tools,
      <link xlink:href="http://xmlbeans.apache.org/docs/2.0.0/guide/tools.html#inst2xsd" xlink:type="simple" xlink:show="new" xlink:actuate="onRequest">http://xmlbeans.apache.org/docs/2.0.0/guide/tools.html#inst2xsd</link>
      (accessed 2012 July 13).
    </bibliomixed></bibliography></article>