<?xml version="1.0" encoding="UTF-8" standalone="no"?><classedDocument><article xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" class="po-hcontainer e0 e0" version="5.0-subset Balisage-1.3" xml:id="Bal2012Dubi0214"><title class="po-block e1 e1"><textual class="po-textual">Exploring the Unknown</textual></title><subtitle class="po-block e2 e2"><textual class="po-textual">Understanding and navigating large XML datasets</textual></subtitle><info class="po-record e3 e3"><confgroup class="po-record e4 e4"><conftitle class="po-field e5 e5"><textual class="po-textual">Balisage: The Markup Conference 2012</textual></conftitle><confdates class="po-field e6 e6"><textual class="po-textual">August 7 - 10, 2012</textual></confdates></confgroup><abstract class="po-container e7 e7"><para class="po-block e8 e8"><textual class="po-textual">Large collections of data are getting published and used more frequently, even by non-statisticians, a situation driven by the mainstreaming
      of big data, linked data, and open data. Often these datasets are in XML
      format, consisting of an unknown set of elements, attributes,
      namespaces, and content models. This paper describes an approach for
      quickly summarizing as well as guiding exploration into a non-indexed
      XML database. Finally, this paper demonstrates a statistical technique
      to approximate faceted search over large datasets, without the need of
      particular index configurations.</textual></para></abstract><author class="po-record e9 e9"><personname class="po-record e10 e10"><firstname class="po-field e11 e11"><textual class="po-textual">Micah</textual></firstname><surname class="po-field e12 e12"><textual class="po-textual">Dubinko</textual></surname></personname><personblurb class="po-container e13 e13"><para class="po-block e14 e14"><textual class="po-textual">Micah Dubinko has worked on diverse projects, from portable
        heart monitors to mobile applications to search engines. He is
        currently Lead Engineer in the Applications group at MarkLogic.</textual></para></personblurb><affiliation class="po-record e15 e15"><jobtitle class="po-field e16 e16"><textual class="po-textual">Lead Engineer</textual></jobtitle><orgname class="po-block e17 e17"><textual class="po-textual">MarkLogic</textual></orgname></affiliation><email class="po-field e18 e18"><textual class="po-textual">Micah.Dubinko@marklogic.com</textual></email></author><legalnotice class="po-container e19 e19"><para class="po-block e20 e20"><textual class="po-textual">Copyright © 2012 Micah Dubinko</textual></para></legalnotice><keywordset class="po-table e21 e21" role="author"><keyword class="po-field e22 e22"><textual class="po-textual">faceted search</textual></keyword><keyword class="po-field e23 e23"><textual class="po-textual">sampling</textual></keyword><keyword class="po-field e24 e24"><textual class="po-textual">statistics</textual></keyword><keyword class="po-field e25 e25"><textual class="po-textual">xml databases</textual></keyword></keywordset></info><section class="po-hcontainer e26 e26"><title class="po-block e27 e27"><textual class="po-textual">More data than you know what to do with</textual></title><para class="po-block e28 e28"><textual class="po-textual">It’s increasingly common for information workers to come in contact
    with unfamiliar datasets. Governments around the world are releasing more
    and bigger datasets. Corporations are collecting and releasing more data
    than ever, often in XML or another format easily convertible to XML. This
    trend, while welcomed by many, poses questions about how to come to
    understand large XML datasets.</textual></para><para class="po-block e29 e29"><textual class="po-textual">In general, bulk analysis of large datasets gets done through an ad-hoc
    assortment of machine learning techniques. Few of these, however, are specifically
    targeted at the unique aspects of the XML data model, as examined in this paper.</textual></para><para class="po-block e30 e30"><textual class="po-textual">One tool in particular that has come into widespread use for analyzing datasets is R,
    which has available a set of XML libraries </textual><xref class="po-milestone e31 e31" linkend="r_xml"><textual class="po-textual">Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao</textual></xref><textual class="po-textual">. However, while R is often useful for
    initial exploration, it lacks an upgrade path into an operational data store such
    as an XML database, and the XML capabilities provided are DOM and XPath-centric,
    which are less suited to bulk analysis.</textual></para><para class="po-block e32 e32"><textual class="po-textual">The most common approach used by databases to deal with searching
    and navigating through large datasets is an index, trading space for time.
    Once an index is set up and occupying additional disk and/or memory space,
    certain kinds of queries run significantly faster.</textual></para><para class="po-block e33 e33"><textual class="po-textual">With an unknown dataset, though, this presents a chicken-and-egg
    problem. Many database systems assume the availability of indexes to
    perform navigational functions such as faceted search. Without such features, it’s more
    difficult to get to know the data. And if the structure of the data is
    unknown, how can one create the necessary indexes in the first
    place?</textual></para><para class="po-block e34 e34"><textual class="po-textual">One answer is that indexes should be arranged based on queries
    rather than data. This is a valid approach, although not of much help in
    the case of bootstrapping a truly unknown dataset.</textual></para><para class="po-block e35 e35"><textual class="po-textual">Another approach, one explored in the remainder of this paper, is to
    rely on statistical sampling approaches rather than indexes. This presents
    tradeoffs in terms of size, speed, and accuracy. At the same time, it
    offers benefits: </textual><variablelist class="po-table e36 e36"><varlistentry class="po-record e37 e37"><term class="po-block e38 e38"><textual class="po-textual">Rapid start</textual></term><listitem class="po-container e39 e39"><para class="po-block e40 e40"><textual class="po-textual">The ability to run queries immediately, without
prior configuration.</textual></para></listitem></varlistentry><varlistentry class="po-record e41 e41"><term class="po-block e42 e42"><textual class="po-textual">Productivity</textual></term><listitem class="po-container e43 e43"><para class="po-block e44 e44"><textual class="po-textual">The ability to run queries before or during a lengthy indexing
            operation.</textual></para></listitem></varlistentry><varlistentry class="po-record e45 e45"><term class="po-block e46 e46"><textual class="po-textual">Exploration</textual></term><listitem class="po-container e47 e47"><para class="po-block e48 e48"><textual class="po-textual">An aid to identifying areas in the dataset that might
            requires some amount of cleanup before applying indexing.</textual></para></listitem></varlistentry><varlistentry class="po-record e49 e49"><term class="po-block e50 e50"><textual class="po-textual">Cross-platform development</textual></term><listitem class="po-container e51 e51"><para class="po-block e52 e52"><textual class="po-textual">None of the techniques here depend on proprietary index
            structures.</textual></para></listitem></varlistentry><varlistentry class="po-record e53 e53"><term class="po-block e54 e54"><textual class="po-textual">Performance baselining</textual></term><listitem class="po-container e55 e55"><para class="po-block e56 e56"><textual class="po-textual">These techniques can be used as a measuring stick to compare
            the size and speed tradeoffs of various index
            configurations.</textual></para></listitem></varlistentry></variablelist></para><para class="po-block e57 e57"><textual class="po-textual">Though the techniques shown here are universally applicable, for
    convenience, code samples in this paper are based on an early access release of MarkLogic 6 </textual><xref class="po-milestone e58 e58" linkend="marklogic"><textual class="po-textual">Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao</textual></xref><textual class="po-textual">.</textual></para></section><section class="po-hcontainer e59 e59"><title class="po-block e60 e60"><textual class="po-textual">Random Sampling</textual></title><para class="po-block e61 e61"><textual class="po-textual">A common approach to characterizing a large population
    involves taking a random sample. A simple XQuery expression </textual><programlisting class="po-block e62 e62" xml:space="preserve"><textual class="po-textual">fn:collection()[1]</textual></programlisting><textual class="po-textual"> takes advantage of the implementation detail that many databases define document order across the entire database in an arbitrary order so that the "first" document is essentially a random choice. One common manual approach is to "eyeball" the first such document, and maybe a few others, to get an intuitive feel for the kinds of data and structures at hand. This approach has obvious scaling difficulties.</textual></para><para class="po-block e63 e63"><textual class="po-textual">For larger samples, greater automation is needed for the analysis. We can define a process
    producing a sample of size N to be considered random if any particular
    collection of documents is equally likely to turn up as any other
    same-sized collection of documents. Randomness of a sample is important, because a non-random sample will lead to systematic errors not accounted for in statistical measures of probability.</textual></para><para class="po-block e64 e64"><textual class="po-textual">Within a random sample it is possible to examine characteristics of
    the sample and make statistical inferences about the overall population. The more
    documents in the sample, the better the approximation. When the overall population is small, it is possible to sample most or even all of the documents. But with large collections of documents, the proportion of documents contained in a reasonably-sized sample will be very small. When dealing with a dataset of this size, certain simplifying assumptions become possible, as outlined in later sections of this paper.</textual></para><para class="po-block e65 e65"><textual class="po-textual">Users of search
    interfaces have become accustomed to approximations, for example, a web
    search engine may report something like “Page 2 of about 415,000,000
    results”, but as users go deeper into the result set, the estimated number
    tends to converge on some actual value. Users have accepted this behavior,
    though it is less commonly used in finer-grained navigational situations. For example, if a sidebar
    on a retail site says, “New in last 90 days (328)”, quite often one will find that
    exactly 328 items will be available by clicking through all the pages of
    results.</textual></para><para class="po-block e66 e66"><textual class="po-textual">This difference in user expectations can be exploited. In particular, in the case of first contact with a new XML dataset,
    exact results are far less important than overall trends and correlations,
    which makes a sampling approach ideal.</textual></para><section class="po-hcontainer e67 e67"><title class="po-block e68 e68"><textual class="po-textual">Sample size end error</textual></title><para class="po-block e69 e69"><textual class="po-textual">Statistical estimates by definition are not completely reliable. For example, it's possible (though breathtakingly unlikely) that a random sample of 100 documents would happen to contain all 100 unusual instances out of database of a million documents. A surer bet, though, would be none of the unusual documents would turn up in a sample of 100. One measure of an estimate's  reliability, called a confidence interval is related to a chosen probability range. To put it another way, if the random experiment were repeated many times whereby it was found that 95.4% of the calculated confidence intervals included the true value, one could say that 95.4% was the confidence interval. (Note, however, that when speaking about a single experiment, the estimated range either contains the true value or it doesn't and one would need more complicated Bayesian techniques to delve deeper.)</textual></para><para class="po-block e70 e70"><textual class="po-textual">For a large population, it is  convenient to use a confidence
interval of 95.4%, which encompasses values two standard deviations from the mean
      in either direction and makes the math come out easier later on. At that confidence level, and assuming a large overall population relative to the sample size, the maximum margin of
      error is simply (continuing to use XQuery notation) </textual><programlisting class="po-block e71 e71" xml:space="preserve"><textual class="po-textual">
1 div math:sqrt($sample-size)
        </textual></programlisting><textual class="po-textual"> although in particular cases, the observed error can
      be somewhat less.</textual></para><para class="po-block e72 e72"><textual class="po-textual">For example, a sample size of 1000, likely to be conveniently held
      in memory, the maximum margin of error comes out to about 3.16%, which
      is good enough for many purposes.</textual></para><para class="po-block e73 e73"><textual class="po-textual">A key weakness in sampling  is that rare values are
      likely to be missed. A doctype that appears only 100 times out of a
      million is unlikely to show up in a sample of 100 documents, and on rare
      occasions when it does show up exactly once, straightforward extrapolation would infer that it occurs in 1% of all documents, a gross overestimation.</textual></para></section><section class="po-hcontainer e74 e74"><title class="po-block e75 e75"><textual class="po-textual">Performance</textual></title><para class="po-block e76 e76"><textual class="po-textual">The approaches in this paper assume that an entire sample of documents can comfortably fit into main memory. To fulfill a random-sampling query without indexes, each document
      needs to be read from disk. Therefore, overall the performance of the query will be roughly
      linear in the sample size, plus time for local processing. Details will vary depending on the database
      system, but in general there will be some amount of data locality making
      for shorter document-to-document disk seeks. A back-of-the-envelope estimate is about 1
      second per 200 documents in the sample, ignoring the prospect of documents cached in memory, perhaps somewhat higher with high-performance disks such as SSD or RAID configurations that stripe data across multiple disks.</textual><footnote class="po-popup e77 e77"><para class="po-block e78 e78"><textual class="po-textual">In general, document-level caching provides little benefit for
          random-sampling based approaches, as a different random set of
          documents gets selected on each run of the experiment.</textual></para></footnote><textual class="po-textual">.</textual></para></section></section><section class="po-hcontainer e79 e79"><title class="po-block e80 e80"><textual class="po-textual">Dipping a toe into the database</textual></title><para class="po-block e81 e81"><textual class="po-textual">Getting familiar with an XML dataset requires a combination of automated and hands-on approaches. While writing this paper, I had on hand a dataset of over 5 million documents, crawled from an assortment of public social media sources,
    of which I knew very little in advance.</textual></para><para class="po-block e82 e82"><textual class="po-textual">The first-document-in-the-collection approach mentioned above yields this
    </textual><footnote class="po-popup e83 e83"><para class="po-block e84 e84"><textual class="po-textual">Namespace URIs have been anonymized.</textual></para></footnote><textual class="po-textual">:</textual></para><para class="po-block e85 e85"><programlisting class="po-block e86 e86" xml:space="preserve"><textual class="po-textual">
&lt;sl:streamlet
xmlns:sl="http://example.com/ns/social-media/streamlet"&gt;
  &lt;sl:vid&gt;13067505336836346999&lt;/sl:vid&gt;
  &lt;sl:tweet&gt;#Jerusalem #News 'Iran cuts funding for
  Hamas due to Syria unrest'
  http://t.co/ARRqabU&lt;/sl:tweet&gt;
&lt;/sl:streamlet&gt;
        </textual></programlisting></para><para class="po-block e87 e87"><textual class="po-textual">The choice of the "first" document, is, of course, completely arbitrary. The second document has something completely different:</textual></para><para class="po-block e88 e88"><programlisting class="po-block e89 e89" xml:space="preserve"><textual class="po-textual">
&lt;person:person
xmlns:person="http://example.com/ns/social-media/person"&gt;  
  &lt;person:id&gt;8999631448253261463&lt;/person:id&gt;
  &lt;person:follower-count&gt;0&lt;/person:follower-count&gt;
  &lt;person:influence&gt;0&lt;/person:influence&gt;
  &lt;person:name&gt;Borana Mukesh&lt;/person:name&gt;
  ...
&lt;/person:person&gt;
        </textual></programlisting></para><para class="po-block e90 e90"><textual class="po-textual">There's only so much one can learn from picking through individual documents.</textual></para><para class="po-block e91 e91"><textual class="po-textual">The following XQuery 3.0 </textual><xref class="po-milestone e92 e92" linkend="xquery"><textual class="po-textual">Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao</textual></xref><textual class="po-textual"> code
    demonstrates this approach by examining a sample of documents in order to extract potentially interesting features, which a human operator can use to make decisions about where to dive deeper. The code assembles the following:</textual></para><itemizedlist class="po-table e93 e93"><listitem class="po-container e94 e94"><para class="po-block e95 e95"><textual class="po-textual">Root elements</textual></para></listitem><listitem class="po-container e96 e96"><para class="po-block e97 e97"><textual class="po-textual">Commonly-occurring elements</textual></para></listitem><listitem class="po-container e98 e98"><para class="po-block e99 e99"><textual class="po-textual">Commonly-occurring namespaces</textual></para></listitem><listitem class="po-container e100 e100"><para class="po-block e101 e101"><textual class="po-textual">Elements that tend to have a lot (or a little) text
        content</textual></para></listitem><listitem class="po-container e102 e102"><para class="po-block e103 e103"><textual class="po-textual">Text nodes that look like dates</textual></para></listitem><listitem class="po-container e104 e104"><para class="po-block e105 e105"><textual class="po-textual">Text nodes that almost look like dates</textual></para></listitem><listitem class="po-container e106 e106"><para class="po-block e107 e107"><textual class="po-textual">Text nodes that look like numeric data, for example years</textual></para></listitem></itemizedlist><programlisting class="po-block e108 e108" xml:space="preserve"><textual class="po-textual">
  let $dv := distinct-values#1
  let $n := ($sample-size, 1000)[1]
  let $xml-docs := spx:est-docs()
  let $text-docs := spx:est-text-docs()

  let $samp := spx:random-sample($n)
  let $cnames := $dv($samp/*/spx:name(.))
  let $all-ns := $dv($samp//namespace::*)
  let $leafe := $samp//*[empty(*)]
  let $leafetxt := $leafe[text()]
  let $leafe-long := $leafetxt
    [string-length(.) ge 10]
  let $leafe-short := $leafetxt
    [string-length(.) le 4]
  let $dates := $dv($leafe-long
    [. castable as xs:dateTime]/spx:name(.))
  let $near-dates := $dv($leafe-long
    [matches(local-name(.), '[Dd]ate')]
    [not(. castable as xs:dateTime)]/spx:name(.))
  let $all-years := $dv($leafe-short
    [matches(., "^(19|20)\d\d$")]/spx:name(.))
  let $all-smallnum := $dv($leafe-short
    [. castable as xs:double]/spx:name(.))
  let $epd := count($samp//*) div count($samp/*)
  return
    &lt;spx:data-sketch
      xml-doc-count="{$xml-docs}"
      text-doc-count="{$text-docs}"
      binary-doc-count="{$binary-docs}"
      elements-per-doc="{$epd}"&gt;
      {$cnames!&lt;spx:root-elem
        name="{.}"
        count="{spx:est-by-QName(spx:QName(.))}"/&gt;
      }
      {$all-ns!&lt;spx:ns-seen&gt;{.}&lt;/spx:ns-seen&gt;}
      {$dates!&lt;spx:date&gt;{.}&lt;/spx:date&gt;}
      {$near-dates!&lt;spx:almost-date&gt;{.}&lt;/spx:almost-date&gt;}
      {$all-years!&lt;spx:year&gt;{.}&lt;/spx:year&gt;}
      {$all-smallnum!&lt;spx:small-num&gt;{.}&lt;/spx:small-num&gt;}
    &lt;/spx:data-sketch&gt;
    </textual></programlisting><para class="po-block e109 e109"><textual class="po-textual">This code and the following listings make use of the following helper functions which
    contain vendor-specific implementations, which are not important here (see the Code section
    later for details):</textual><variablelist class="po-table e110 e110"><varlistentry class="po-record e111 e111"><term class="po-block e112 e112"><textual class="po-textual">spx:est-docs()</textual></term><listitem class="po-container e113 e113"><para class="po-block e114 e114"><textual class="po-textual">A function that quickly estimates the total number of documents in the database.</textual></para></listitem></varlistentry><varlistentry class="po-record e115 e115"><term class="po-block e116 e116"><textual class="po-textual">spx:est-test-docs()</textual></term><listitem class="po-container e117 e117"><para class="po-block e118 e118"><textual class="po-textual">A function that quickly estimates the total number of documents in the database that consist of a single text node.</textual></para></listitem></varlistentry><varlistentry class="po-record e119 e119"><term class="po-block e120 e120"><textual class="po-textual">spx:random-sample()</textual></term><listitem class="po-container e121 e121"><para class="po-block e122 e122"><textual class="po-textual">A function that returns a random sample of documents from the database.</textual></para></listitem></varlistentry><varlistentry class="po-record e123 e123"><term class="po-block e124 e124"><textual class="po-textual">spx:name()</textual></term><listitem class="po-container e125 e125"><para class="po-block e126 e126"><textual class="po-textual">Returns a Clark name of a given node.</textual></para></listitem></varlistentry><varlistentry class="po-record e127 e127"><term class="po-block e128 e128"><textual class="po-textual">spx:formatq()</textual></term><listitem class="po-container e129 e129"><para class="po-block e130 e130"><textual class="po-textual">Returns a Clark name from a given QName.</textual></para></listitem></varlistentry><varlistentry class="po-record e131 e131"><term class="po-block e132 e132"><textual class="po-textual">spx:node-path()</textual></term><listitem class="po-container e133 e133"><para class="po-block e134 e134"><textual class="po-textual">Returns an XPath expression that uniquely identifies a node.</textual></para></listitem></varlistentry></variablelist></para><para class="po-block e135 e135"><textual class="po-textual">This code
    produced the following (with adjustments for line length):</textual><programlisting class="po-block e136 e136" xml:space="preserve"><textual class="po-textual">
&lt;spx:data-sketch
xmlns:spx="http://dubinko.info/spelunx"
  xml-doc-count="5789128"
  text-doc-count="0"
  binary-doc-count="0"
  elements-per-doc="12.88" &gt;
  &lt;spx:root-elem name="{...}person" count="1248848"/&gt;
  &lt;spx:root-elem name="{...}media" count="2117625"/&gt;
  &lt;spx:root-elem name="{...}streamlet" count="1173545"/&gt;
  &lt;spx:root-elem name="{...}author" count="1248815"/&gt;
  &lt;spx:ns-seen&gt;
    http://example.com/ns/social-media/person
  &lt;/spx:ns-seen&gt;
  &lt;spx:ns-seen&gt;
    http://example.com/ns/social-media/media
  &lt;/spx:ns-seen&gt;
  &lt;spx:ns-seen&gt;
    http://example.com/ns/social-media/streamlet
  &lt;/spx:ns-seen&gt;
  &lt;spx:ns-seen&gt;
    http://example.com/ns/social-media/author
  &lt;/spx:ns-seen&gt;
  &lt;spx:ns-seen&gt;
    http://www.w3.org/XML/1998/namespace
  &lt;/spx:ns-seen&gt;
  &lt;spx:date&gt;{...}ingested&lt;/spx:date&gt;
  &lt;spx:date&gt;{...}published&lt;/spx:date&gt;
  &lt;spx:date&gt;{...}canonical&lt;/spx:date&gt;
  &lt;spx:date&gt;{...}inserted&lt;/spx:date&gt;
  &lt;spx:small-num&gt;{...}follower-count&lt;/spx:small-num&gt;
  &lt;spx:small-num&gt;{...}influence&lt;/spx:small-num&gt;
  &lt;spx:small-num&gt;{...}follower-count&lt;/spx:small-num&gt;
&lt;/spx:data-sketch&gt;
    </textual></programlisting></para><para class="po-block e137 e137"><textual class="po-textual">This dataset appears fairly homogeneous: only four different root element
    QNames, were observed over 1,000 samples. Additionally, these documents
    contain a number of elements that seem date-like, but would require some
    cleanup in order to be represented in as the Schema datatype xs:dateTime.
    For purposes of this paper, one particular element, </textual><code class="po-atom e138 e138"><textual class="po-textual">influence</textual></code><textual class="po-textual">, 
    as seen earlier, seems particularly
    interesting. Is there a way to learn more about it?</textual></para></section><section class="po-hcontainer e139 e139"><title class="po-block e140 e140"><textual class="po-textual">Digging Deeper</textual></title><para class="po-block e141 e141"><textual class="po-textual">It’s possible to perform similar kinds of analysis on specific nodes
    in the database. Given a starting node, the system of XPath axes provides
    a number of different ways in which to characterize that element’s use in
    a larger dataset. Some care must be taken to handle edge cases, assuming
    nothing in an unknown environment. The following code listing
    characterizes a given element node (named with a QName) along several
    important axes:</textual><programlisting class="po-block e142 e142" xml:space="preserve"><textual class="po-textual">
  let $dv := distinct-values#1
  let $n := ($sample-size, 1000)[1]
  let $samp := spx:random-sample($n)

  let $ocrs := $samp//*[node-name(.) eq $e]
  let $vals := data($ocrs)
  let $number-vals := $vals
    [. castable as xs:double]
  let $nv := $number-vals
  let $date-values := $vals
    [. castable as xs:dateTime]
  let $blank-vals := $vals[matches(., "^\s*$")]
  let $parents := $dv(
    $ocrs/node-name(..)!spx:formatq(.))
  let $children := $dv($ocrs/*!spx:name(.))
  let $attrs := $dv($ocrs/@*!spx:name(.))
  let $roots := $dv($ocrs/root()/*!spx:name(.))
  let $paths := $dv($ocrs/spx:node-path(.))
  return
    &lt;spx:node-report
      estimate-count="{spx:est-by-QName($e)}"
      sample-count="{count($ocrs)}"
      number-count="{count($number-vals)}"
      date-count="{count($date-values)}"
      blank-count="{count($blank-vals)}"&gt;
      {$parents!&lt;spx:parent&gt;{.}&lt;/spx:parent&gt;}
      {$roots!&lt;spx:root&gt;{.}&lt;/spx:root&gt;}
      {$paths!&lt;spx:path&gt;{.}&lt;/spx:path&gt;}
      &lt;spx:min&gt;{min($number-vals)}&lt;/spx:min&gt;
      &lt;spx:max&gt;{max($number-vals)}&lt;/spx:max&gt;
      {if (exists($vals)) then
      &lt;spx:mean&gt;
        {sum($nv) div count($nv)}
      &lt;/spx:mean&gt;
      else ()
      }
    &lt;/spx:node-report&gt;
    </textual></programlisting></para><para class="po-block e143 e143"><textual class="po-textual">These two techniques combine to provide a powerful tool for picking
    through an unknown dataset. First identify ‘interesting’ element nodes,
    then dig into each one to see how it is used in the data. While the sample
    documents are in memory, it is possible to infer datatype information, and
    for values that look numeric, to calculate the sample min, max, mean,
    median, standard deviation, and other useful statistics.</textual></para><para class="po-block e144 e144"><textual class="po-textual">These techniques can be readily expanded to include statistics for
    other node types, notably attribute and processing-instruction
    nodes.</textual></para></section><section class="po-hcontainer e145 e145"><title class="po-block e146 e146"><textual class="po-textual">Free-form faceting</textual></title><para class="po-block e147 e147"><textual class="po-textual">Index-backed approaches make it possible to produce a histogram of values,
    often called "facets", for example all the prices in a product database,
    arranged into buckets of values like 'less than $10' or '$10 to $50'
    and so on.</textual></para><para class="po-block e148 e148"><textual class="po-textual">It’s possible to combine the concepts introduced thus far by breaking down a random sample into faceted data.
    With no advance knowledge of the range of values, it’s difficult to arrange values into
    reasonable buckets, but with some spelunking, as in the preceding section,
    it’s possible to construct reasonable bucketing. Based on the exploration
    from the preceding sections, the </textual><code class="po-atom e149 e149"><textual class="po-textual">influence</textual></code><textual class="po-textual"> element looks
    worth further investigation.</textual></para><para class="po-block e150 e150"><textual class="po-textual">The following XQuery function plots out the values of a given
    element as xs:double values in specified ranges.</textual><programlisting class="po-block e151 e151" xml:space="preserve"><textual class="po-textual">
declare function spx:histogram(
  $e as xs:QName,
  $sample-size as  xs:unsignedInt?,
  $bounds as xs:double+
) {
  let $n := ($sample-size, 1000)[1]
  let $samp := spx:random-sample($n)
  let $full-population := spx:est-docs()
  let $multiplier := ($full-population div $n)
  let $ocrs := $samp//*[node-name(.) eq $e]
  let $vals := data($ocrs)
  let $number-vals := $vals
    [. castable as xs:double]!xs:double(.)
  let $bucket-tops := ($bounds, xs:float("INF"))
  for $bucket-top at $idx in $bucket-tops
  let $bucket-bottom :=
    if ($idx eq 1)
    then xs:float("-INF")
    else $bucket-tops[position() eq $idx - 1]
  let $samp-count := count($number-vals
    [. lt $bucket-top][. ge $bucket-bottom])
  let $p := $samp-count div $n
  let $moe := 1 div math:sqrt($sample-size)
  let $SE := math:sqrt(($p * (1 - $p)) div $n)
  let $est-count := $samp-count * $multiplier
  let $error := $SE * $full-population
  let $est-top := $est-count + $error
  let $est-bot := $est-count - $error
  return
    &lt;histogram-value
      ge="{$bucket-bottom}"
      lt="{$bucket-top}"
      sample-count="{$samp-count}"
      est-count="{$est-count}"
      est-range="{$est-bot} to {$est-top}"
      error="{$error}"/&gt;
};
    </textual></programlisting></para><para class="po-block e152 e152"><textual class="po-textual">This code accepts a particular QName referring to an element, a
    sample size, and an ordered set of numeric bounds, and returns the
    approximate count of values that occur in between each boundary. The first
    bucket includes values down to </textual><code class="po-atom e153 e153"><textual class="po-textual">-INF</textual></code><textual class="po-textual">, and the last bucket
    includes all values up to </textual><code class="po-atom e154 e154"><textual class="po-textual">INF</textual></code><textual class="po-textual">. Selecting values to partition
    the values into order-of-magnitude buckets will give a broad first
    approximation of the distribution.</textual></para><para class="po-block e155 e155"><textual class="po-textual">For comparison, the following vendor-specific code, which requires a
    pre-existing in-memory index, resolves the exact counts of different
    values occurring in the database.</textual></para><programlisting class="po-block e156 e156" xml:space="preserve"><textual class="po-textual">
for $bucket in cts:element-value-ranges(
  QName("http://example.com/ns/social-media/person", "influence"),
  (1,10,100,1000), "empties")
return
    &lt;histogram-value
        ge="{($bucket/cts:lower-bound, '-INF')[1]}"
        lt="{($bucket/cts:upper-bound, 'INF')[1]}"
        count="{cts:frequency($bucket)}"/&gt;
    </textual></programlisting><para class="po-block e157 e157"><textual class="po-textual">The results of calling these function on the test database are given
    below in table format.</textual></para></section><section class="po-hcontainer e158 e158"><title class="po-block e159 e159"><textual class="po-textual">How wrong can you get?</textual></title><para class="po-block e160 e160"><textual class="po-textual">As the book Statistics Hacks </textual><xref class="po-milestone e161 e161" linkend="statshacks"><textual class="po-textual">Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao</textual></xref><textual class="po-textual"> states,
    </textual><quote class="po-inline e162 e162"><textual class="po-textual">Anytime you have used statistics to summarize observations, you’ve probably been wrong.</textual></quote><textual class="po-textual"> This technique is no exception.</textual></para><para class="po-block e163 e163"><textual class="po-textual">As mentioned earlier, if we assume that the sample is of a small
    proportion of the overall population and is randomly selected, the maximum
    margin of error is a simple function of sample size. However, against
    particular values we can usually find a more accurate estimate.</textual></para><para class="po-block e164 e164"><textual class="po-textual">To estimate the overall proportion, the standard error of the
    proportion must be computed, using the following formula.</textual><programlisting class="po-block e165 e165" xml:space="preserve"><textual class="po-textual">math:sqrt(($p - $p * $p) div $sample-size )</textual></programlisting><textual class="po-textual">The
    maximum error, which occurs when the proportion is exactly 50%, is exactly
    half of the margin of error calculation earlier</textual><footnote class="po-popup e166 e166"><para class="po-block e167 e167"><textual class="po-textual">Half because margin of error already accounts for error in the
        plus or minus direction, i.e. a diameter, while standard error or the
        proportion does not, i.e. a radius.</textual></para></footnote><textual class="po-textual">.</textual></para><para class="po-block e168 e168"><textual class="po-textual">The following table illustrates the tradeoffs in accuracy, run-time,
    and necessity of a preconfigured index. The columns on the left represent histogram buckets of values of various ranges,
    while the rows across the top represent different sample sizes, or in the case of the final column, exact index resolution.
    The hardware under test consisted of a 2.4Ghz Dual-core 64-bit Intel machine with two 15k rpm disks
    in a RAID O configuration.</textual></para><table border="1" class="po-container e169 e169"><caption class="po-container e170 e170"><para class="po-block e171 e171"><textual class="po-textual">Comparison of run-time and accuracy</textual></para></caption><thead class="po-container e172 e172"><tr class="po-table e173 e173"><th class="po-field e174 e174"><textual class="po-textual">Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao</textual></th><th class="po-field e175 e175"><textual class="po-textual">estimate, n=10</textual></th><th class="po-field e176 e176"><textual class="po-textual">estimate, n=100</textual></th><th class="po-field e177 e177"><textual class="po-textual">estimate, n=1000</textual></th><th class="po-field e178 e178"><textual class="po-textual">estimate, n=10000</textual></th><th class="po-field e179 e179"><textual class="po-textual">exact index resolution</textual></th></tr></thead><tbody class="po-table e180 e180"><tr class="po-table e181 e181"><td class="po-block e182 e182"><textual class="po-textual">values &lt; 1</textual></td><td class="po-block e183 e183"><textual class="po-textual">115,7825</textual></td><td class="po-block e184 e184"><textual class="po-textual">810,477</textual></td><td class="po-block e185 e185"><textual class="po-textual">897,314</textual></td><td class="po-block e186 e186"><textual class="po-textual">782,111</textual></td><td class="po-block e187 e187"><textual class="po-textual">807,284</textual></td></tr><tr class="po-table e188 e188"><td class="po-block e189 e189"><textual class="po-textual">1 &lt;= values &lt; 10</textual></td><td class="po-block e190 e190"><textual class="po-textual">0</textual></td><td class="po-block e191 e191"><textual class="po-textual">0</textual></td><td class="po-block e192 e192"><textual class="po-textual">17,367</textual></td><td class="po-block e193 e193"><textual class="po-textual">25,472</textual></td><td class="po-block e194 e194"><textual class="po-textual">28,414</textual></td></tr><tr class="po-table e195 e195"><td class="po-block e196 e196"><textual class="po-textual">10 &lt;= values &lt; 100</textual></td><td class="po-block e197 e197"><textual class="po-textual">578,912</textual></td><td class="po-block e198 e198"><textual class="po-textual">115,782</textual></td><td class="po-block e199 e199"><textual class="po-textual">208,408</textual></td><td class="po-block e200 e200"><textual class="po-textual">164,990</textual></td><td class="po-block e201 e201"><textual class="po-textual">161,734</textual></td></tr><tr class="po-table e202 e202"><td class="po-block e203 e203"><textual class="po-textual">100 &lt;= values &lt; 1000</textual></td><td class="po-block e204 e204"><textual class="po-textual">578,912</textual></td><td class="po-block e205 e205"><textual class="po-textual">347,347</textual></td><td class="po-block e206 e206"><textual class="po-textual">219,986</textual></td><td class="po-block e207 e207"><textual class="po-textual">208,408</textual></td><td class="po-block e208 e208"><textual class="po-textual">204,298</textual></td></tr><tr class="po-table e209 e209"><td class="po-block e210 e210"><textual class="po-textual">values &gt;= 1000</textual></td><td class="po-block e211 e211"><textual class="po-textual">0</textual></td><td class="po-block e212 e212"><textual class="po-textual">0</textual></td><td class="po-block e213 e213"><textual class="po-textual">28,945</textual></td><td class="po-block e214 e214"><textual class="po-textual">36,471</textual></td><td class="po-block e215 e215"><textual class="po-textual">47,070</textual></td></tr><tr class="po-table e216 e216"><th class="po-field e217 e217"><textual class="po-textual">Run time</textual></th><td class="po-block e218 e218"><textual class="po-textual">0.35 sec</textual></td><td class="po-block e219 e219"><textual class="po-textual">0.48 sec</textual></td><td class="po-block e220 e220"><textual class="po-textual">1.9 sec</textual></td><td class="po-block e221 e221"><textual class="po-textual">19.4 sec</textual></td><td class="po-block e222 e222"><textual class="po-textual">0.19 sec</textual></td></tr></tbody></table><para class="po-block e223 e223"><textual class="po-textual">Unsurprisingly, at smaller sample sizes, it is probable that
    infequently-occurring data will be completely excluded from the random
    sample. Even the most frequently-occurring values, in this case the bucket
    of values less than one, occurs in less than 14% of the 5.7M documents.
    Given this, the accuracy of the random sampling technique, even at the
    lower sample counts, is more than enough to give a general impression of
    the distribution of the data values.</textual></para><para class="po-block e224 e224"><textual class="po-textual">To visulaize this, it is possible to export these values into a
    desktop spreadsheet program and produce a graph, including error bars, as
    shown in the following figure.</textual></para><figure class="po-container e225 e225"><title class="po-block e226 e226"><textual class="po-textual">Graphical representation of data distribution (by percentage)</textual></title><mediaobject class="po-container e227 e227"><imageobject class="po-container e228 e228"><imagedata class="po-meta e229 e229" fileref="../../../vol8/graphics/Dubinko01/Dubinko01-001.png" format="png"><textual class="po-textual">Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao</textual></imagedata></imageobject></mediaobject></figure></section><section class="po-hcontainer e230 e230"><title class="po-block e231 e231"><textual class="po-textual">Conclusion</textual></title><para class="po-block e232 e232"><textual class="po-textual">The techniques shown in the paper offer a useful framework within which to
    make the initial foray into an unknown XML dataset. Starting with an automated
    run-down of high-level features in the dataset, particular QNames chosen by the
    user can be drilled down into deeper analysis. The dataset can even be summarized
    through histogram facets, much like those available to significantly more
    resource-intensive indexed databases.</textual></para><para class="po-block e233 e233"><textual class="po-textual">The techniques shown here do not rely on proprietary features and are
    applicable to a wide range of available XQuery processors.</textual></para><para class="po-block e234 e234"><textual class="po-textual">The book How to Lie with Statistics </textual><xref class="po-milestone e235 e235" linkend="statslie"><textual class="po-textual">Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao</textual></xref><textual class="po-textual">
    concludes with advice on how to be properly skeptical of statistics, and
    the guidelines apply to the techniques in this paper as much as in any
    other area.</textual></para><variablelist class="po-table e236 e236"><varlistentry class="po-record e237 e237"><term class="po-block e238 e238"><textual class="po-textual">What's missing?</textual></term><listitem class="po-container e239 e239"><para class="po-block e240 e240"><textual class="po-textual">Be on the lookout for areas where summarization may be
          obscuring important facts.</textual></para></listitem></varlistentry><varlistentry class="po-record e241 e241"><term class="po-block e242 e242"><textual class="po-textual">Did somebody change the subject?</textual></term><listitem class="po-container e243 e243"><para class="po-block e244 e244"><textual class="po-textual">Beware of an unfounded jump from raw figures to
          conclusions.</textual></para></listitem></varlistentry><varlistentry class="po-record e245 e245"><term class="po-block e246 e246"><textual class="po-textual">Does it make sense?</textual></term><listitem class="po-container e247 e247"><para class="po-block e248 e248"><textual class="po-textual">Any results that come from these techniques need to be
          eyeballed. Anything that seems wildly out of proportion needs to be
          more closely examined.</textual></para></listitem></varlistentry></variablelist><para class="po-block e249 e249"><textual class="po-textual">With those caveats, the techniques in this paper can provide a
    useful lever by which to pry open a large, unkonwn XML data set.</textual></para></section><section class="po-hcontainer e250 e250"><title class="po-block e251 e251"><textual class="po-textual">Code availability</textual></title><para class="po-block e252 e252"><textual class="po-textual">The code samples mentioned in this paper are in available in a 
    project named Spelunx available at GitHub </textual><xref class="po-milestone e253 e253" linkend="spelunx"><textual class="po-textual">Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao Pippo! Ciao</textual></xref><textual class="po-textual">.</textual></para></section><section class="po-hcontainer e254 e254"><title class="po-block e255 e255"><textual class="po-textual">Further topics to explore</textual></title><itemizedlist class="po-table e256 e256"><listitem class="po-container e257 e257"><para class="po-block e258 e258"><textual class="po-textual">Correlation and co-existence between given nodes</textual></para></listitem><listitem class="po-container e259 e259"><para class="po-block e260 e260"><textual class="po-textual">Multi-dimensional sampling, as in geographic data</textual></para></listitem><listitem class="po-container e261 e261"><para class="po-block e262 e262"><textual class="po-textual">Searching for correlated latitude and longitude pairs</textual></para></listitem><listitem class="po-container e263 e263"><para class="po-block e264 e264"><textual class="po-textual">Hypothesis testing and type I vs type II errors</textual></para></listitem><listitem class="po-container e265 e265"><para class="po-block e266 e266"><textual class="po-textual">Markov chain analysis for element and attribute
        containership</textual></para></listitem><listitem class="po-container e267 e267"><para class="po-block e268 e268"><textual class="po-textual">Comparison and contrast with machine learning techniques</textual></para></listitem><listitem class="po-container e269 e269"><para class="po-block e270 e270"><textual class="po-textual">Exploring the availability of random-sampling extension
        functions from different database vendors</textual></para></listitem><listitem class="po-container e271 e271"><para class="po-block e272 e272"><textual class="po-textual">Ways to summarize mixed content</textual></para></listitem></itemizedlist></section><bibliography class="po-hcontainer e273 e273"><title class="po-block e274 e274"><textual class="po-textual">Bibliography</textual></title><bibliomixed class="po-block e275 e275" xml:id="r_xml" xreflabel="R Language; Package 'XML'"><textual class="po-textual">Duncan Temple Lang (editor), </textual><quote class="po-inline e276 e276"><textual class="po-textual">Package 'XML', version 3.9-4</textual></quote><textual class="po-textual"> [online]. [cited 13th July, 2012]. </textual><link class="po-inline e277 e277" xlink:actuate="onRequest" xlink:show="new" xlink:type="simple"><textual class="po-textual">http://cran.r-project.org/web/packages/XML/XML.pdf</textual></link></bibliomixed><bibliomixed class="po-block e278 e278" xml:id="xquery" xreflabel="XQuery 3.0"><textual class="po-textual">Jonathan Robie, Don Chamberlin, Michael Dyck, and John Snelson (editors), </textual><quote class="po-inline e279 e279"><textual class="po-textual">XQuery 3.0: An XML Query Language</textual></quote><textual class="po-textual"> [online]. [cited 13th July 2012]. </textual><link class="po-inline e280 e280" xlink:actuate="onRequest" xlink:show="new" xlink:type="simple"><textual class="po-textual">http://www.w3.org/TR/2011/WD-xquery-30-20111213/</textual></link></bibliomixed><bibliomixed class="po-block e281 e281" xml:id="marklogic" xreflabel="MarkLogic docs"><textual class="po-textual">MarkLogic Corporation, </textual><quote class="po-inline e282 e282"><textual class="po-textual">MarkLogic Server Search Developer's Guide</textual></quote><textual class="po-textual"> [online]. © 2012 [cited 13th July 2012]. </textual><link class="po-inline e283 e283" xlink:actuate="onRequest" xlink:show="new" xlink:type="simple"><textual class="po-textual">http://developer.marklogic.com/pubs/5.0/books/search-dev-guide.pdf</textual></link></bibliomixed><bibliomixed class="po-block e284 e284" xml:id="clarknotation" xreflabel="Clark notation"><textual class="po-textual">James Clark, </textual><quote class="po-inline e285 e285"><textual class="po-textual">XML Namespaces</textual></quote><textual class="po-textual"> [online]. [cited 13th July, 2012]. James describes "universal names written as a URI in curly brackets followed by the local name" which have proved to be a useful construction in more contexts than explaining namespaces. </textual><link class="po-inline e286 e286" xlink:actuate="onRequest" xlink:show="new" xlink:type="simple"><textual class="po-textual">http://www.jclark.com/xml/xmlns.htm</textual></link></bibliomixed><bibliomixed class="po-block e287 e287" xml:id="statshacks" xreflabel="Statistics Hacks"><textual class="po-textual">Bruce Frey, </textual><quote class="po-inline e288 e288"><textual class="po-textual">Statistics Hacks: Tips &amp; Tools for Measuring the World and Beating the Odds</textual></quote><textual class="po-textual"> (O'Reilly Media, 2006). Despite the name, includes a great deal of basic information on statistics and the math behind it.</textual></bibliomixed><bibliomixed class="po-block e289 e289" xml:id="statslie" xreflabel="How to Lie with Statistics"><textual class="po-textual">Darrell Huff, </textual><quote class="po-inline e290 e290"><textual class="po-textual">How to Lie with Statistics</textual></quote><textual class="po-textual"> (W. W. Norton &amp; Company, 1993 reprint). To appreciate the power of statistics, you must understand how it can be used as a weapon.</textual></bibliomixed><bibliomixed class="po-block e291 e291" xml:id="spelunx" xreflabel="Spelunx"><textual class="po-textual">Micah Dubinko, </textual><quote class="po-inline e292 e292"><textual class="po-textual">Spelunx</textual></quote><textual class="po-textual"> open source project [online] </textual><link class="po-inline e293 e293" xlink:actuate="onRequest" xlink:show="new" xlink:type="simple"><textual class="po-textual">https://github.com/mdubinko/spelunx</textual></link></bibliomixed></bibliography></article></classedDocument>