<div id="mainContainerTOC">
   <div id="mainContainerTitleTOC" onclick="$('#mainContainerEntriesTOC').toggle('1000');">Table of Content</div>
   <div id="mainContainerEntriesTOC" style="display:none;">
      <div class="headedContainerTOC">
         <div class="headedContainerTitleTOC"><a href="#ExploringtheUnknownANCHOR" name="ExploringtheUnknownTOC">Exploring the Unknown</a></div>
         <div class="headedContainerTOC">
            <div class="headedContainerTitleTOC"><a href="#MoredatathanyouknowwhattodowithANCHOR" name="MoredatathanyouknowwhattodowithTOC">More data than you know what to do with</a></div>
         </div>
         <div class="headedContainerTOC">
            <div class="headedContainerTitleTOC"><a href="#RandomSamplingANCHOR" name="RandomSamplingTOC">Random Sampling</a></div>
            <div class="headedContainerTOC">
               <div class="headedContainerTitleTOC"><a href="#SamplesizeenderrorANCHOR" name="SamplesizeenderrorTOC">Sample size end error</a></div>
            </div>
            <div class="headedContainerTOC">
               <div class="headedContainerTitleTOC"><a href="#PerformanceANCHOR" name="PerformanceTOC">Performance</a></div>
            </div>
         </div>
         <div class="headedContainerTOC">
            <div class="headedContainerTitleTOC"><a href="#DippingatoeintothedatabaseANCHOR" name="DippingatoeintothedatabaseTOC">Dipping a toe into the database</a></div>
         </div>
         <div class="headedContainerTOC">
            <div class="headedContainerTitleTOC"><a href="#DiggingDeeperANCHOR" name="DiggingDeeperTOC">Digging Deeper</a></div>
         </div>
         <div class="headedContainerTOC">
            <div class="headedContainerTitleTOC"><a href="#FreeformfacetingANCHOR" name="FreeformfacetingTOC">Free-form faceting</a></div>
         </div>
         <div class="headedContainerTOC">
            <div class="headedContainerTitleTOC"><a href="#Howwrongcanyouget?ANCHOR" name="Howwrongcanyouget?TOC">How wrong can you get?</a></div>
         </div>
         <div class="headedContainerTOC">
            <div class="headedContainerTitleTOC"><a href="#ConclusionANCHOR" name="ConclusionTOC">Conclusion</a></div>
         </div>
         <div class="headedContainerTOC">
            <div class="headedContainerTitleTOC"><a href="#CodeavailabilityANCHOR" name="CodeavailabilityTOC">Code availability</a></div>
         </div>
         <div class="headedContainerTOC">
            <div class="headedContainerTitleTOC"><a href="#FurthertopicstoexploreANCHOR" name="FurthertopicstoexploreTOC">Further topics to explore</a></div>
         </div>
         <div class="headedContainerTOC">
            <div class="headedContainerTitleTOC"><a href="#BibliographyANCHOR" name="BibliographyTOC">Bibliography</a></div>
         </div>
      </div>
   </div>
</div>
<div id="mainContainerTERMS">
   <div id="mainContainerTitleTERMS" onclick="$('#mainContainerEntriesTERMS').toggle('1000')">Index of Terms</div>
   <div id="mainContainerEntriesTERMS" style="display:none;">
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">a</div>
         <div class="singletermTERMS">August 7 - 10, 2012</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">b</div>
         <div class="singletermTERMS">Balisage: The Markup Conference 2012</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">c</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">d</div>
         <div class="singletermTERMS">Dubinko</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">e</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">f</div>
         <div class="singletermTERMS">faceted search</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">g</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">h</div>
         <div class="singletermTERMS">https://github.com/mdubinko/spelunx</div>
         <div class="singletermTERMS">How to Lie with Statistics</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">i</div>
         <div class="singletermTERMS">influence</div>
         <div class="singletermTERMS">INF</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">j</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">k</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">l</div>
         <div class="singletermTERMS">Lead Engineer</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">m</div>
         <div class="singletermTERMS">MarkLogic Server Search Developer's Guide</div>
         <div class="singletermTERMS">Micah.Dubinko@marklogic.com</div>
         <div class="singletermTERMS">Micah</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">n</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">o</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">p</div>
         <div class="singletermTERMS">Package 'XML', version 3.9-4</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">q</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">r</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">s</div>
         <div class="singletermTERMS">Spelunx</div>
         <div class="singletermTERMS">sampling</div>
         <div class="singletermTERMS">statistics</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">t</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">u</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">v</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">x</div>
         <div class="singletermTERMS">XQuery 3.0: An XML Query Language</div>
         <div class="singletermTERMS">XML Namespaces</div>
         <div class="singletermTERMS">xml databases</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">y</div>
      </div>
      <div class="letterContainerTERMS">
         <div class="letterlableTERMS">z</div>
      </div>
   </div>
</div>
<div id="mainContainerIML"><a name="ExploringtheUnknownANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="article e0" version="5.0-subset Balisage-1.3" xml:id="Bal2012Dubi0214">
      <div class="title e1">Exploring the Unknown</div>
      <div class="subtitle e2">Understanding and navigating large XML datasets</div>
      <div class="info e3">
         <div class="confgroup e4">
            <div class="conftitle e5">Balisage: The Markup Conference 2012</div>
            <div class="confdates e6">August 7 - 10, 2012</div>
         </div>
         <div class="abstract e7">
            <div class="para e8">Large collections of data are getting published and used more frequently, even by
               non-statisticians, a situation driven by the mainstreaming
               of big data, linked data, and open data. Often these datasets are in XML
               format, consisting of an unknown set of elements, attributes,
               namespaces, and content models. This paper describes an approach for
               quickly summarizing as well as guiding exploration into a non-indexed
               XML database. Finally, this paper demonstrates a statistical technique
               to approximate faceted search over large datasets, without the need of
               particular index configurations.
            </div>
         </div>
         <div class="author e9">
            <div class="personname e10">
               <div class="firstname e11">Micah</div>
               <div class="surname e12">Dubinko</div>
            </div>
            <div class="personblurb e13">
               <div class="para e14">Micah Dubinko has worked on diverse projects, from portable
                  heart monitors to mobile applications to search engines. He is
                  currently Lead Engineer in the Applications group at MarkLogic.
               </div>
            </div>
            <div class="affiliation e15">
               <div class="jobtitle e16">Lead Engineer</div>
               <div class="orgname e17">MarkLogic</div>
            </div>
            <div class="email e18">Micah.Dubinko@marklogic.com</div>
         </div>
         <div class="legalnotice e19">
            <div class="para e20">Copyright © 2012 Micah Dubinko</div>
         </div>
         <div class="keywordset e21" role="author">
            <div class="keyword e22">faceted search</div>
            <div class="keyword e23">sampling</div>
            <div class="keyword e24">statistics</div>
            <div class="keyword e25">xml databases</div>
         </div>
      </div><a name="MoredatathanyouknowwhattodowithANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="section e26">
         <div class="title e27">More data than you know what to do with</div>
         <div class="para e28">It’s increasingly common for information workers to come in contact
            with unfamiliar datasets. Governments around the world are releasing more
            and bigger datasets. Corporations are collecting and releasing more data
            than ever, often in XML or another format easily convertible to XML. This
            trend, while welcomed by many, poses questions about how to come to
            understand large XML datasets.
         </div>
         <div class="para e29">In general, bulk analysis of large datasets gets done through an ad-hoc
            assortment of machine learning techniques. Few of these, however, are specifically
            targeted at the unique aspects of the XML data model, as examined in this paper.
         </div>
         <div class="para e30">One tool in particular that has come into widespread use for analyzing datasets is
            R,
            which has available a set of XML libraries 
            <div class="xref e31" linkend="r_xml"></div>. However, while R is often useful for
            initial exploration, it lacks an upgrade path into an operational data store such
            as an XML database, and the XML capabilities provided are DOM and XPath-centric,
            which are less suited to bulk analysis.
         </div>
         <div class="para e32">The most common approach used by databases to deal with searching
            and navigating through large datasets is an index, trading space for time.
            Once an index is set up and occupying additional disk and/or memory space,
            certain kinds of queries run significantly faster.
         </div>
         <div class="para e33">With an unknown dataset, though, this presents a chicken-and-egg
            problem. Many database systems assume the availability of indexes to
            perform navigational functions such as faceted search. Without such features, it’s
            more
            difficult to get to know the data. And if the structure of the data is
            unknown, how can one create the necessary indexes in the first
            place?
         </div>
         <div class="para e34">One answer is that indexes should be arranged based on queries
            rather than data. This is a valid approach, although not of much help in
            the case of bootstrapping a truly unknown dataset.
         </div>
         <div class="para e35">Another approach, one explored in the remainder of this paper, is to
            rely on statistical sampling approaches rather than indexes. This presents
            tradeoffs in terms of size, speed, and accuracy. At the same time, it
            offers benefits: 
            <div class="variablelist e36">
               <div class="varlistentry e37">
                  <div class="term e38">Rapid start</div>
                  <div class="listitem e39">
                     <div class="para e40">The ability to run queries immediately, without
                        prior configuration.
                     </div>
                  </div>
               </div>
               <div class="varlistentry e41">
                  <div class="term e42">Productivity</div>
                  <div class="listitem e43">
                     <div class="para e44">The ability to run queries before or during a lengthy indexing
                        operation.
                     </div>
                  </div>
               </div>
               <div class="varlistentry e45">
                  <div class="term e46">Exploration</div>
                  <div class="listitem e47">
                     <div class="para e48">An aid to identifying areas in the dataset that might
                        requires some amount of cleanup before applying indexing.
                     </div>
                  </div>
               </div>
               <div class="varlistentry e49">
                  <div class="term e50">Cross-platform development</div>
                  <div class="listitem e51">
                     <div class="para e52">None of the techniques here depend on proprietary index
                        structures.
                     </div>
                  </div>
               </div>
               <div class="varlistentry e53">
                  <div class="term e54">Performance baselining</div>
                  <div class="listitem e55">
                     <div class="para e56">These techniques can be used as a measuring stick to compare
                        the size and speed tradeoffs of various index
                        configurations.
                     </div>
                  </div>
               </div>
            </div>
         </div>
         <div class="para e57">Though the techniques shown here are universally applicable, for
            convenience, code samples in this paper are based on an early access release of MarkLogic
            6 
            <div class="xref e58" linkend="marklogic"></div>.
         </div>
      </div><a name="RandomSamplingANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="section e59">
         <div class="title e60">Random Sampling</div>
         <div class="para e61">A common approach to characterizing a large population
            involves taking a random sample. A simple XQuery expression 
            <div class="programlisting e62" xml:space="preserve">fn:collection()[1]</div> takes advantage of the implementation detail that many databases define document
            order across the entire database in an arbitrary order so that the "first" document
            is essentially a random choice. One common manual approach is to "eyeball" the first
            such document, and maybe a few others, to get an intuitive feel for the kinds of data
            and structures at hand. This approach has obvious scaling difficulties.
         </div>
         <div class="para e63">For larger samples, greater automation is needed for the analysis. We can define a
            process
            producing a sample of size N to be considered random if any particular
            collection of documents is equally likely to turn up as any other
            same-sized collection of documents. Randomness of a sample is important, because a
            non-random sample will lead to systematic errors not accounted for in statistical
            measures of probability.
         </div>
         <div class="para e64">Within a random sample it is possible to examine characteristics of
            the sample and make statistical inferences about the overall population. The more
            documents in the sample, the better the approximation. When the overall population
            is small, it is possible to sample most or even all of the documents. But with large
            collections of documents, the proportion of documents contained in a reasonably-sized
            sample will be very small. When dealing with a dataset of this size, certain simplifying
            assumptions become possible, as outlined in later sections of this paper.
         </div>
         <div class="para e65">Users of search
            interfaces have become accustomed to approximations, for example, a web
            search engine may report something like “Page 2 of about 415,000,000
            results”, but as users go deeper into the result set, the estimated number
            tends to converge on some actual value. Users have accepted this behavior,
            though it is less commonly used in finer-grained navigational situations. For example,
            if a sidebar
            on a retail site says, “New in last 90 days (328)”, quite often one will find that
            exactly 328 items will be available by clicking through all the pages of
            results.
         </div>
         <div class="para e66">This difference in user expectations can be exploited. In particular, in the case
            of first contact with a new XML dataset,
            exact results are far less important than overall trends and correlations,
            which makes a sampling approach ideal.
         </div><a name="SamplesizeenderrorANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="section e67">
            <div class="title e68">Sample size end error</div>
            <div class="para e69">Statistical estimates by definition are not completely reliable. For example, it's
               possible (though breathtakingly unlikely) that a random sample of 100 documents would
               happen to contain all 100 unusual instances out of database of a million documents.
               A surer bet, though, would be none of the unusual documents would turn up in a sample
               of 100. One measure of an estimate's  reliability, called a confidence interval is
               related to a chosen probability range. To put it another way, if the random experiment
               were repeated many times whereby it was found that 95.4% of the calculated confidence
               intervals included the true value, one could say that 95.4% was the confidence interval.
               (Note, however, that when speaking about a single experiment, the estimated range
               either contains the true value or it doesn't and one would need more complicated Bayesian
               techniques to delve deeper.)
            </div>
            <div class="para e70">For a large population, it is  convenient to use a confidence
               interval of 95.4%, which encompasses values two standard deviations from the mean
               in either direction and makes the math come out easier later on. At that confidence
               level, and assuming a large overall population relative to the sample size, the maximum
               margin of
               error is simply (continuing to use XQuery notation) 
               <div class="programlisting e71" xml:space="preserve">
                  1 div math:sqrt($sample-size)
                  
               </div> although in particular cases, the observed error can
               be somewhat less.
            </div>
            <div class="para e72">For example, a sample size of 1000, likely to be conveniently held
               in memory, the maximum margin of error comes out to about 3.16%, which
               is good enough for many purposes.
            </div>
            <div class="para e73">A key weakness in sampling  is that rare values are
               likely to be missed. A doctype that appears only 100 times out of a
               million is unlikely to show up in a sample of 100 documents, and on rare
               occasions when it does show up exactly once, straightforward extrapolation would infer
               that it occurs in 1% of all documents, a gross overestimation.
            </div>
         </div><a name="PerformanceANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="section e74">
            <div class="title e75">Performance</div>
            <div class="para e76">The approaches in this paper assume that an entire sample of documents can comfortably
               fit into main memory. To fulfill a random-sampling query without indexes, each document
               needs to be read from disk. Therefore, overall the performance of the query will be
               roughly
               linear in the sample size, plus time for local processing. Details will vary depending
               on the database
               system, but in general there will be some amount of data locality making
               for shorter document-to-document disk seeks. A back-of-the-envelope estimate is about
               1
               second per 200 documents in the sample, ignoring the prospect of documents cached
               in memory, perhaps somewhat higher with high-performance disks such as SSD or RAID
               configurations that stripe data across multiple disks.
               <div class="popupBox e77">
                  <div class="popupLabel" onmouseover="$('#d1e132').show('1000');" onmouseout="$('#d1e132').hide('1000');">[ footnote ]</div>
                  <div id="d1e132" style="display: none;">
                     <div class="footnote">
                        <div class="para e78">In general, document-level caching provides little benefit for
                           random-sampling based approaches, as a different random set of
                           documents gets selected on each run of the experiment.
                        </div>
                     </div>
                  </div>
               </div>.
            </div>
         </div>
      </div><a name="DippingatoeintothedatabaseANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="section e79">
         <div class="title e80">Dipping a toe into the database</div>
         <div class="para e81">Getting familiar with an XML dataset requires a combination of automated and hands-on
            approaches. While writing this paper, I had on hand a dataset of over 5 million documents,
            crawled from an assortment of public social media sources,
            of which I knew very little in advance.
         </div>
         <div class="para e82">The first-document-in-the-collection approach mentioned above yields this
            
            <div class="popupBox e83">
               <div class="popupLabel" onmouseover="$('#d1e143').show('1000');" onmouseout="$('#d1e143').hide('1000');">[ footnote ]</div>
               <div id="d1e143" style="display: none;">
                  <div class="footnote">
                     <div class="para e84">Namespace URIs have been anonymized.</div>
                  </div>
               </div>
            </div>:
         </div>
         <div class="para e85">
            <div class="programlisting e86" xml:space="preserve">
               &lt;sl:streamlet
               xmlns:sl="http://example.com/ns/social-media/streamlet"&gt;
               &lt;sl:vid&gt;13067505336836346999&lt;/sl:vid&gt;
               &lt;sl:tweet&gt;#Jerusalem #News 'Iran cuts funding for
               Hamas due to Syria unrest'
               http://t.co/ARRqabU&lt;/sl:tweet&gt;
               &lt;/sl:streamlet&gt;
               
            </div>
         </div>
         <div class="para e87">The choice of the "first" document, is, of course, completely arbitrary. The second
            document has something completely different:
         </div>
         <div class="para e88">
            <div class="programlisting e89" xml:space="preserve">
               &lt;person:person
               xmlns:person="http://example.com/ns/social-media/person"&gt;  
               &lt;person:id&gt;8999631448253261463&lt;/person:id&gt;
               &lt;person:follower-count&gt;0&lt;/person:follower-count&gt;
               &lt;person:influence&gt;0&lt;/person:influence&gt;
               &lt;person:name&gt;Borana Mukesh&lt;/person:name&gt;
               ...
               &lt;/person:person&gt;
               
            </div>
         </div>
         <div class="para e90">There's only so much one can learn from picking through individual documents.</div>
         <div class="para e91">The following XQuery 3.0 
            <div class="xref e92" linkend="xquery"></div> code
            demonstrates this approach by examining a sample of documents in order to extract
            potentially interesting features, which a human operator can use to make decisions
            about where to dive deeper. The code assembles the following:
         </div>
         <div class="itemizedlist e93">
            <div class="listitem e94">
               <div class="para e95">Root elements</div>
            </div>
            <div class="listitem e96">
               <div class="para e97">Commonly-occurring elements</div>
            </div>
            <div class="listitem e98">
               <div class="para e99">Commonly-occurring namespaces</div>
            </div>
            <div class="listitem e100">
               <div class="para e101">Elements that tend to have a lot (or a little) text
                  content
               </div>
            </div>
            <div class="listitem e102">
               <div class="para e103">Text nodes that look like dates</div>
            </div>
            <div class="listitem e104">
               <div class="para e105">Text nodes that almost look like dates</div>
            </div>
            <div class="listitem e106">
               <div class="para e107">Text nodes that look like numeric data, for example years</div>
            </div>
         </div>
         <div class="programlisting e108" xml:space="preserve">
            let $dv := distinct-values#1
            let $n := ($sample-size, 1000)[1]
            let $xml-docs := spx:est-docs()
            let $text-docs := spx:est-text-docs()
            
            let $samp := spx:random-sample($n)
            let $cnames := $dv($samp/*/spx:name(.))
            let $all-ns := $dv($samp//namespace::*)
            let $leafe := $samp//*[empty(*)]
            let $leafetxt := $leafe[text()]
            let $leafe-long := $leafetxt
            [string-length(.) ge 10]
            let $leafe-short := $leafetxt
            [string-length(.) le 4]
            let $dates := $dv($leafe-long
            [. castable as xs:dateTime]/spx:name(.))
            let $near-dates := $dv($leafe-long
            [matches(local-name(.), '[Dd]ate')]
            [not(. castable as xs:dateTime)]/spx:name(.))
            let $all-years := $dv($leafe-short
            [matches(., "^(19|20)\d\d$")]/spx:name(.))
            let $all-smallnum := $dv($leafe-short
            [. castable as xs:double]/spx:name(.))
            let $epd := count($samp//*) div count($samp/*)
            return
            &lt;spx:data-sketch
            xml-doc-count="{$xml-docs}"
            text-doc-count="{$text-docs}"
            binary-doc-count="{$binary-docs}"
            elements-per-doc="{$epd}"&gt;
            {$cnames!&lt;spx:root-elem
            name="{.}"
            count="{spx:est-by-QName(spx:QName(.))}"/&gt;
            }
            {$all-ns!&lt;spx:ns-seen&gt;{.}&lt;/spx:ns-seen&gt;}
            {$dates!&lt;spx:date&gt;{.}&lt;/spx:date&gt;}
            {$near-dates!&lt;spx:almost-date&gt;{.}&lt;/spx:almost-date&gt;}
            {$all-years!&lt;spx:year&gt;{.}&lt;/spx:year&gt;}
            {$all-smallnum!&lt;spx:small-num&gt;{.}&lt;/spx:small-num&gt;}
            &lt;/spx:data-sketch&gt;
            
         </div>
         <div class="para e109">This code and the following listings make use of the following helper functions which
            contain vendor-specific implementations, which are not important here (see the Code
            section
            later for details):
            <div class="variablelist e110">
               <div class="varlistentry e111">
                  <div class="term e112">spx:est-docs()</div>
                  <div class="listitem e113">
                     <div class="para e114">A function that quickly estimates the total number of documents in the database.</div>
                  </div>
               </div>
               <div class="varlistentry e115">
                  <div class="term e116">spx:est-test-docs()</div>
                  <div class="listitem e117">
                     <div class="para e118">A function that quickly estimates the total number of documents in the database that
                        consist of a single text node.
                     </div>
                  </div>
               </div>
               <div class="varlistentry e119">
                  <div class="term e120">spx:random-sample()</div>
                  <div class="listitem e121">
                     <div class="para e122">A function that returns a random sample of documents from the database.</div>
                  </div>
               </div>
               <div class="varlistentry e123">
                  <div class="term e124">spx:name()</div>
                  <div class="listitem e125">
                     <div class="para e126">Returns a Clark name of a given node.</div>
                  </div>
               </div>
               <div class="varlistentry e127">
                  <div class="term e128">spx:formatq()</div>
                  <div class="listitem e129">
                     <div class="para e130">Returns a Clark name from a given QName.</div>
                  </div>
               </div>
               <div class="varlistentry e131">
                  <div class="term e132">spx:node-path()</div>
                  <div class="listitem e133">
                     <div class="para e134">Returns an XPath expression that uniquely identifies a node.</div>
                  </div>
               </div>
            </div>
         </div>
         <div class="para e135">This code
            produced the following (with adjustments for line length):
            <div class="programlisting e136" xml:space="preserve">
               &lt;spx:data-sketch
               xmlns:spx="http://dubinko.info/spelunx"
               xml-doc-count="5789128"
               text-doc-count="0"
               binary-doc-count="0"
               elements-per-doc="12.88" &gt;
               &lt;spx:root-elem name="{...}person" count="1248848"/&gt;
               &lt;spx:root-elem name="{...}media" count="2117625"/&gt;
               &lt;spx:root-elem name="{...}streamlet" count="1173545"/&gt;
               &lt;spx:root-elem name="{...}author" count="1248815"/&gt;
               &lt;spx:ns-seen&gt;
               http://example.com/ns/social-media/person
               &lt;/spx:ns-seen&gt;
               &lt;spx:ns-seen&gt;
               http://example.com/ns/social-media/media
               &lt;/spx:ns-seen&gt;
               &lt;spx:ns-seen&gt;
               http://example.com/ns/social-media/streamlet
               &lt;/spx:ns-seen&gt;
               &lt;spx:ns-seen&gt;
               http://example.com/ns/social-media/author
               &lt;/spx:ns-seen&gt;
               &lt;spx:ns-seen&gt;
               http://www.w3.org/XML/1998/namespace
               &lt;/spx:ns-seen&gt;
               &lt;spx:date&gt;{...}ingested&lt;/spx:date&gt;
               &lt;spx:date&gt;{...}published&lt;/spx:date&gt;
               &lt;spx:date&gt;{...}canonical&lt;/spx:date&gt;
               &lt;spx:date&gt;{...}inserted&lt;/spx:date&gt;
               &lt;spx:small-num&gt;{...}follower-count&lt;/spx:small-num&gt;
               &lt;spx:small-num&gt;{...}influence&lt;/spx:small-num&gt;
               &lt;spx:small-num&gt;{...}follower-count&lt;/spx:small-num&gt;
               &lt;/spx:data-sketch&gt;
               
            </div>
         </div>
         <div class="para e137">This dataset appears fairly homogeneous: only four different root element
            QNames, were observed over 1,000 samples. Additionally, these documents
            contain a number of elements that seem date-like, but would require some
            cleanup in order to be represented in as the Schema datatype xs:dateTime.
            For purposes of this paper, one particular element, 
            <div class="code e138">influence</div>, 
            as seen earlier, seems particularly
            interesting. Is there a way to learn more about it?
         </div>
      </div><a name="DiggingDeeperANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="section e139">
         <div class="title e140">Digging Deeper</div>
         <div class="para e141">It’s possible to perform similar kinds of analysis on specific nodes
            in the database. Given a starting node, the system of XPath axes provides
            a number of different ways in which to characterize that element’s use in
            a larger dataset. Some care must be taken to handle edge cases, assuming
            nothing in an unknown environment. The following code listing
            characterizes a given element node (named with a QName) along several
            important axes:
            <div class="programlisting e142" xml:space="preserve">
               let $dv := distinct-values#1
               let $n := ($sample-size, 1000)[1]
               let $samp := spx:random-sample($n)
               
               let $ocrs := $samp//*[node-name(.) eq $e]
               let $vals := data($ocrs)
               let $number-vals := $vals
               [. castable as xs:double]
               let $nv := $number-vals
               let $date-values := $vals
               [. castable as xs:dateTime]
               let $blank-vals := $vals[matches(., "^\s*$")]
               let $parents := $dv(
               $ocrs/node-name(..)!spx:formatq(.))
               let $children := $dv($ocrs/*!spx:name(.))
               let $attrs := $dv($ocrs/@*!spx:name(.))
               let $roots := $dv($ocrs/root()/*!spx:name(.))
               let $paths := $dv($ocrs/spx:node-path(.))
               return
               &lt;spx:node-report
               estimate-count="{spx:est-by-QName($e)}"
               sample-count="{count($ocrs)}"
               number-count="{count($number-vals)}"
               date-count="{count($date-values)}"
               blank-count="{count($blank-vals)}"&gt;
               {$parents!&lt;spx:parent&gt;{.}&lt;/spx:parent&gt;}
               {$roots!&lt;spx:root&gt;{.}&lt;/spx:root&gt;}
               {$paths!&lt;spx:path&gt;{.}&lt;/spx:path&gt;}
               &lt;spx:min&gt;{min($number-vals)}&lt;/spx:min&gt;
               &lt;spx:max&gt;{max($number-vals)}&lt;/spx:max&gt;
               {if (exists($vals)) then
               &lt;spx:mean&gt;
               {sum($nv) div count($nv)}
               &lt;/spx:mean&gt;
               else ()
               }
               &lt;/spx:node-report&gt;
               
            </div>
         </div>
         <div class="para e143">These two techniques combine to provide a powerful tool for picking
            through an unknown dataset. First identify ‘interesting’ element nodes,
            then dig into each one to see how it is used in the data. While the sample
            documents are in memory, it is possible to infer datatype information, and
            for values that look numeric, to calculate the sample min, max, mean,
            median, standard deviation, and other useful statistics.
         </div>
         <div class="para e144">These techniques can be readily expanded to include statistics for
            other node types, notably attribute and processing-instruction
            nodes.
         </div>
      </div><a name="FreeformfacetingANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="section e145">
         <div class="title e146">Free-form faceting</div>
         <div class="para e147">Index-backed approaches make it possible to produce a histogram of values,
            often called "facets", for example all the prices in a product database,
            arranged into buckets of values like 'less than $10' or '$10 to $50'
            and so on.
         </div>
         <div class="para e148">It’s possible to combine the concepts introduced thus far by breaking down a random
            sample into faceted data.
            With no advance knowledge of the range of values, it’s difficult to arrange values
            into
            reasonable buckets, but with some spelunking, as in the preceding section,
            it’s possible to construct reasonable bucketing. Based on the exploration
            from the preceding sections, the 
            <div class="code e149">influence</div> element looks
            worth further investigation.
         </div>
         <div class="para e150">The following XQuery function plots out the values of a given
            element as xs:double values in specified ranges.
            <div class="programlisting e151" xml:space="preserve">
               declare function spx:histogram(
               $e as xs:QName,
               $sample-size as  xs:unsignedInt?,
               $bounds as xs:double+
               ) {
               let $n := ($sample-size, 1000)[1]
               let $samp := spx:random-sample($n)
               let $full-population := spx:est-docs()
               let $multiplier := ($full-population div $n)
               let $ocrs := $samp//*[node-name(.) eq $e]
               let $vals := data($ocrs)
               let $number-vals := $vals
               [. castable as xs:double]!xs:double(.)
               let $bucket-tops := ($bounds, xs:float("INF"))
               for $bucket-top at $idx in $bucket-tops
               let $bucket-bottom :=
               if ($idx eq 1)
               then xs:float("-INF")
               else $bucket-tops[position() eq $idx - 1]
               let $samp-count := count($number-vals
               [. lt $bucket-top][. ge $bucket-bottom])
               let $p := $samp-count div $n
               let $moe := 1 div math:sqrt($sample-size)
               let $SE := math:sqrt(($p * (1 - $p)) div $n)
               let $est-count := $samp-count * $multiplier
               let $error := $SE * $full-population
               let $est-top := $est-count + $error
               let $est-bot := $est-count - $error
               return
               &lt;histogram-value
               ge="{$bucket-bottom}"
               lt="{$bucket-top}"
               sample-count="{$samp-count}"
               est-count="{$est-count}"
               est-range="{$est-bot} to {$est-top}"
               error="{$error}"/&gt;
               };
               
            </div>
         </div>
         <div class="para e152">This code accepts a particular QName referring to an element, a
            sample size, and an ordered set of numeric bounds, and returns the
            approximate count of values that occur in between each boundary. The first
            bucket includes values down to 
            <div class="code e153">-INF</div>, and the last bucket
            includes all values up to 
            <div class="code e154">INF</div>. Selecting values to partition
            the values into order-of-magnitude buckets will give a broad first
            approximation of the distribution.
         </div>
         <div class="para e155">For comparison, the following vendor-specific code, which requires a
            pre-existing in-memory index, resolves the exact counts of different
            values occurring in the database.
         </div>
         <div class="programlisting e156" xml:space="preserve">
            for $bucket in cts:element-value-ranges(
            QName("http://example.com/ns/social-media/person", "influence"),
            (1,10,100,1000), "empties")
            return
            &lt;histogram-value
            ge="{($bucket/cts:lower-bound, '-INF')[1]}"
            lt="{($bucket/cts:upper-bound, 'INF')[1]}"
            count="{cts:frequency($bucket)}"/&gt;
            
         </div>
         <div class="para e157">The results of calling these function on the test database are given
            below in table format.
         </div>
      </div><a name="Howwrongcanyouget?ANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="section e158">
         <div class="title e159">How wrong can you get?</div>
         <div class="para e160">As the book Statistics Hacks 
            <div class="xref e161" linkend="statshacks"></div> states,
            
            <div class="quote e162">Anytime you have used statistics to summarize observations, you’ve probably been wrong.</div> This technique is no exception.
         </div>
         <div class="para e163">As mentioned earlier, if we assume that the sample is of a small
            proportion of the overall population and is randomly selected, the maximum
            margin of error is a simple function of sample size. However, against
            particular values we can usually find a more accurate estimate.
         </div>
         <div class="para e164">To estimate the overall proportion, the standard error of the
            proportion must be computed, using the following formula.
            <div class="programlisting e165" xml:space="preserve">math:sqrt(($p - $p * $p) div $sample-size )</div>The
            maximum error, which occurs when the proportion is exactly 50%, is exactly
            half of the margin of error calculation earlier
            <div class="popupBox e166">
               <div class="popupLabel" onmouseover="$('#d1e290').show('1000');" onmouseout="$('#d1e290').hide('1000');">[ footnote ]</div>
               <div id="d1e290" style="display: none;">
                  <div class="footnote">
                     <div class="para e167">Half because margin of error already accounts for error in the
                        plus or minus direction, i.e. a diameter, while standard error or the
                        proportion does not, i.e. a radius.
                     </div>
                  </div>
               </div>
            </div>.
         </div>
         <div class="para e168">The following table illustrates the tradeoffs in accuracy, run-time,
            and necessity of a preconfigured index. The columns on the left represent histogram
            buckets of values of various ranges,
            while the rows across the top represent different sample sizes, or in the case of
            the final column, exact index resolution.
            The hardware under test consisted of a 2.4Ghz Dual-core 64-bit Intel machine with
            two 15k rpm disks
            in a RAID O configuration.
         </div>
         <div class="table e169" border="1">
            <div class="caption e170">
               <div class="para e171">Comparison of run-time and accuracy</div>
            </div>
            <div class="thead e172">
               <div class="tr e173">
                  <div class="th e174"></div>
                  <div class="th e175">estimate, n=10</div>
                  <div class="th e176">estimate, n=100</div>
                  <div class="th e177">estimate, n=1000</div>
                  <div class="th e178">estimate, n=10000</div>
                  <div class="th e179">exact index resolution</div>
               </div>
            </div>
            <div class="tbody e180">
               <div class="tr e181">
                  <div class="td e182">values &lt; 1</div>
                  <div class="td e183">115,7825</div>
                  <div class="td e184">810,477</div>
                  <div class="td e185">897,314</div>
                  <div class="td e186">782,111</div>
                  <div class="td e187">807,284</div>
               </div>
               <div class="tr e188">
                  <div class="td e189">1 &lt;= values &lt; 10</div>
                  <div class="td e190">0</div>
                  <div class="td e191">0</div>
                  <div class="td e192">17,367</div>
                  <div class="td e193">25,472</div>
                  <div class="td e194">28,414</div>
               </div>
               <div class="tr e195">
                  <div class="td e196">10 &lt;= values &lt; 100</div>
                  <div class="td e197">578,912</div>
                  <div class="td e198">115,782</div>
                  <div class="td e199">208,408</div>
                  <div class="td e200">164,990</div>
                  <div class="td e201">161,734</div>
               </div>
               <div class="tr e202">
                  <div class="td e203">100 &lt;= values &lt; 1000</div>
                  <div class="td e204">578,912</div>
                  <div class="td e205">347,347</div>
                  <div class="td e206">219,986</div>
                  <div class="td e207">208,408</div>
                  <div class="td e208">204,298</div>
               </div>
               <div class="tr e209">
                  <div class="td e210">values &gt;= 1000</div>
                  <div class="td e211">0</div>
                  <div class="td e212">0</div>
                  <div class="td e213">28,945</div>
                  <div class="td e214">36,471</div>
                  <div class="td e215">47,070</div>
               </div>
               <div class="tr e216">
                  <div class="th e217">Run time</div>
                  <div class="td e218">0.35 sec</div>
                  <div class="td e219">0.48 sec</div>
                  <div class="td e220">1.9 sec</div>
                  <div class="td e221">19.4 sec</div>
                  <div class="td e222">0.19 sec</div>
               </div>
            </div>
         </div>
         <div class="para e223">Unsurprisingly, at smaller sample sizes, it is probable that
            infequently-occurring data will be completely excluded from the random
            sample. Even the most frequently-occurring values, in this case the bucket
            of values less than one, occurs in less than 14% of the 5.7M documents.
            Given this, the accuracy of the random sampling technique, even at the
            lower sample counts, is more than enough to give a general impression of
            the distribution of the data values.
         </div>
         <div class="para e224">To visulaize this, it is possible to export these values into a
            desktop spreadsheet program and produce a graph, including error bars, as
            shown in the following figure.
         </div>
         <div class="figure e225">
            <div class="title e226">Graphical representation of data distribution (by percentage)</div>
            <div class="mediaobject e227">
               <div class="imageobject e228">
                  <div class="metaBox e229"><img src="resources/manbluflag.jpg" class="metaImage" onclick="$('#d1e401').toggle('1000');" alt="Marker: click here for details..." title="Marker: click here for details..."><div id="d1e401" class="metaSource" style="display:none;">                
                        &lt;imagedata<span class="metaAttribute">class='e229'
                           </span><span class="metaAttribute">fileref='../../../vol8/graphics/Dubinko01/Dubinko01-001.png'
                           </span><span class="metaAttribute">format='png'
                           </span>
                        &gt;
                        
                     </div>
                  </div>
               </div>
            </div>
         </div>
      </div><a name="ConclusionANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="section e230">
         <div class="title e231">Conclusion</div>
         <div class="para e232">The techniques shown in the paper offer a useful framework within which to
            make the initial foray into an unknown XML dataset. Starting with an automated
            run-down of high-level features in the dataset, particular QNames chosen by the
            user can be drilled down into deeper analysis. The dataset can even be summarized
            through histogram facets, much like those available to significantly more
            resource-intensive indexed databases.
         </div>
         <div class="para e233">The techniques shown here do not rely on proprietary features and are
            applicable to a wide range of available XQuery processors.
         </div>
         <div class="para e234">The book How to Lie with Statistics 
            <div class="xref e235" linkend="statslie"></div>
            concludes with advice on how to be properly skeptical of statistics, and
            the guidelines apply to the techniques in this paper as much as in any
            other area.
         </div>
         <div class="variablelist e236">
            <div class="varlistentry e237">
               <div class="term e238">What's missing?</div>
               <div class="listitem e239">
                  <div class="para e240">Be on the lookout for areas where summarization may be
                     obscuring important facts.
                  </div>
               </div>
            </div>
            <div class="varlistentry e241">
               <div class="term e242">Did somebody change the subject?</div>
               <div class="listitem e243">
                  <div class="para e244">Beware of an unfounded jump from raw figures to
                     conclusions.
                  </div>
               </div>
            </div>
            <div class="varlistentry e245">
               <div class="term e246">Does it make sense?</div>
               <div class="listitem e247">
                  <div class="para e248">Any results that come from these techniques need to be
                     eyeballed. Anything that seems wildly out of proportion needs to be
                     more closely examined.
                  </div>
               </div>
            </div>
         </div>
         <div class="para e249">With those caveats, the techniques in this paper can provide a
            useful lever by which to pry open a large, unkonwn XML data set.
         </div>
      </div><a name="CodeavailabilityANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="section e250">
         <div class="title e251">Code availability</div>
         <div class="para e252">The code samples mentioned in this paper are in available in a 
            project named Spelunx available at GitHub 
            <div class="xref e253" linkend="spelunx"></div>.
         </div>
      </div><a name="FurthertopicstoexploreANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="section e254">
         <div class="title e255">Further topics to explore</div>
         <div class="itemizedlist e256">
            <div class="listitem e257">
               <div class="para e258">Correlation and co-existence between given nodes</div>
            </div>
            <div class="listitem e259">
               <div class="para e260">Multi-dimensional sampling, as in geographic data</div>
            </div>
            <div class="listitem e261">
               <div class="para e262">Searching for correlated latitude and longitude pairs</div>
            </div>
            <div class="listitem e263">
               <div class="para e264">Hypothesis testing and type I vs type II errors</div>
            </div>
            <div class="listitem e265">
               <div class="para e266">Markov chain analysis for element and attribute
                  containership
               </div>
            </div>
            <div class="listitem e267">
               <div class="para e268">Comparison and contrast with machine learning techniques</div>
            </div>
            <div class="listitem e269">
               <div class="para e270">Exploring the availability of random-sampling extension
                  functions from different database vendors
               </div>
            </div>
            <div class="listitem e271">
               <div class="para e272">Ways to summarize mixed content</div>
            </div>
         </div>
      </div><a name="BibliographyANCHOR" href="#mainContainerTitleTOC" class="anchor">toc</a><div class="bibliography e273">
         <div class="title e274">Bibliography</div>
         <div class="bibliomixed e275" xml:id="r_xml" xreflabel="R Language; Package 'XML'">Duncan Temple Lang (editor), 
            <div class="quote e276">Package 'XML', version 3.9-4</div> [online]. [cited 13th July, 2012]. 
            <div xmlns:xlink="http://www.w3.org/1999/xlink" class="link e277" xlink:actuate="onRequest" xlink:show="new" xlink:type="simple">http://cran.r-project.org/web/packages/XML/XML.pdf</div>
         </div>
         <div class="bibliomixed e278" xml:id="xquery" xreflabel="XQuery 3.0">Jonathan Robie, Don Chamberlin, Michael Dyck, and John Snelson (editors), 
            <div class="quote e279">XQuery 3.0: An XML Query Language</div> [online]. [cited 13th July 2012]. 
            <div xmlns:xlink="http://www.w3.org/1999/xlink" class="link e280" xlink:actuate="onRequest" xlink:show="new" xlink:type="simple">http://www.w3.org/TR/2011/WD-xquery-30-20111213/</div>
         </div>
         <div class="bibliomixed e281" xml:id="marklogic" xreflabel="MarkLogic docs">MarkLogic Corporation, 
            <div class="quote e282">MarkLogic Server Search Developer's Guide</div> [online]. © 2012 [cited 13th July 2012]. 
            <div xmlns:xlink="http://www.w3.org/1999/xlink" class="link e283" xlink:actuate="onRequest" xlink:show="new" xlink:type="simple">http://developer.marklogic.com/pubs/5.0/books/search-dev-guide.pdf</div>
         </div>
         <div class="bibliomixed e284" xml:id="clarknotation" xreflabel="Clark notation">James Clark, 
            <div class="quote e285">XML Namespaces</div> [online]. [cited 13th July, 2012]. James describes "universal names written as a
            URI in curly brackets followed by the local name" which have proved to be a useful
            construction in more contexts than explaining namespaces. 
            <div xmlns:xlink="http://www.w3.org/1999/xlink" class="link e286" xlink:actuate="onRequest" xlink:show="new" xlink:type="simple">http://www.jclark.com/xml/xmlns.htm</div>
         </div>
         <div class="bibliomixed e287" xml:id="statshacks" xreflabel="Statistics Hacks">Bruce Frey, 
            <div class="quote e288">Statistics Hacks: Tips &amp; Tools for Measuring the World and Beating the Odds</div> (O'Reilly Media, 2006). Despite the name, includes a great deal of basic information
            on statistics and the math behind it.
         </div>
         <div class="bibliomixed e289" xml:id="statslie" xreflabel="How to Lie with Statistics">Darrell Huff, 
            <div class="quote e290">How to Lie with Statistics</div> (W. W. Norton &amp; Company, 1993 reprint). To appreciate the power of statistics, you
            must understand how it can be used as a weapon.
         </div>
         <div class="bibliomixed e291" xml:id="spelunx" xreflabel="Spelunx">Micah Dubinko, 
            <div class="quote e292">Spelunx</div> open source project [online] 
            <div xmlns:xlink="http://www.w3.org/1999/xlink" class="link e293" xlink:actuate="onRequest" xlink:show="new" xlink:type="simple">https://github.com/mdubinko/spelunx</div>
         </div>
      </div>
   </div>
</div>